






딕셔너리(Dictionaries)

딕셔너리(dictionary)는 리스트 같지만 좀더 일반적이다. 
리스트에서 위치(인텍스)는 정수이지만, 딕셔너리에서는 인덱스는 임의 자료형(type)이 될 수 있다.

딕셔너리를 인덱스 집합(키(keys)라고 부름)에서 값(value) 집합으로 사상(mapping)하는 것으로 생각할 수 있다. 
각각의 키는 값에 대응한다. 
키와 값을 연관시키는 것을 키-값 페어(key-value pair)라고 부르고, 종종 항목(item)으로도 부른다.

한 예제로, 영어 단어에서 스페인 단어에 매핑되는 사전을 만들 것이다. 
키와 값은 모두 문자열이다.

dict 함수는 항목이 전혀 없는 사전을 신규로 생성한다. 
dict는 내장함수명이어서, 변수명으로 사용하는 것을 피해야 한다.


구불구불한 괄호 {}는 빈 딕셔너리를 나타낸다. 
딕셔너리에 항목을 추가하기 위해서 꺾쇠 괄호를 사용한다.




상기 라인은 키 'one'에서 값 'uno'를 매핑하는 항목을 생성한다. 
딕셔너리를 다시 출력하면, 키와 값 사이에 콜론(:)을 가진 키-값 페어(key-value pair)를 볼 수 있다.




출력 형식이 또한 입력 형식이다. 
예를 들어, 세개 항목을 가진 신규 딕셔너리를 생성할 수 있다.




eng2sp을 출력하면, 놀랄 것이다.




키-값 페어(key-value pair) 순서가 같지 않다. 
사실 동일한 사례를 여러분의 컴퓨터에서 입력하면, 다른 결과를 얻게 된다.
일반적으로, 딕셔너리 항목 순서는 예측 가능하지 않다.

딕셔너리 요소가 정수 인덱스로 색인되지 않아서 문제되지는 않는다.
대신에, 키를 사용해서 상응하는 값을 찾을 수 있다.




'two' 키는 항상 값 'dos'에 상응되어서 딕셔너리 항목 순서는 문제가 되지 않는다.

만약 키가 딕셔너리에 존재하지 않으면, 예외 오류가 발생한다.




len 함수를 딕셔너리에 사용해서, 키-값 페어(key-value pair) 항목 개수를 반환한다.




in 연산자도 딕셔너리에 작동되는데, 어떤 것이 딕셔너리 키(key)에 있는지 알려준다. (값(value)으로 나타내는 것은 충분히 좋지는 않다.)




딕셔너리에 무엇이 값으로 있는지 확인하기 위해서, values 메쏘드를 해서 리스트로 값을 반환받고 나서 in 연산자를 사용하여 확인한다.  




in 연산자는 리스트와 딕셔너리에 각기 다른 알고리즘을 사용한다. 
리스트에 대해서 선형 검색 알고리즘을 사용한다.
리스트가 길어짐에 따라 검색 시간은 리스트 길이에 비례하여 길어진다. 
딕셔너리에 대해서 파이썬은 해쉬 테이블(hash table)로 불리는 놀라운 특성을 가진 알고리즘을 사용한다. 
얼마나 많은 항목이 딕셔너리에 있는지에 관계없이 in 연산자는 대략 동일한 시간이 소요된다.
왜 해쉬 함수가 마술 같은지에 대해서는 설명하지 않지만, wikipedia.org/wiki/Hash_table 에서 좀더 많은 정보를 얻을 수 있다. 


words.txt 단어를 읽어서 딕셔너리에 키로 저장하는 프로그램을 작성하세요.
값이 무엇이든지 상관없습니다. 
딕셔너리에 문자열을 확인하는 가장 빠른 방법으로 in 연산자를 사용할 수 있습니다.


계수기(counter) 집합으로서 딕셔너리
문자열이 주어진 상태에서, 각 문자가 얼마나 나타나는지를 센다고 가정합시다.
몇 가지 방법이 아래에 있습니다.



26개 변수를 알파벳 문자 각각에 대해 생성한다. 
그리고 나서 아마도 연쇄 조건문을 사용하여 문자열을 훑고 해당 계수기(counter)를 하나씩 증가한다.

26개 요소를 가진 리스트를 생성한다. 
내장함수 ord를 사용해서 각 문자를 숫자로 변환한다. 
리스트 안에 인덱스로 숫자를 사용해서 적당한 계수기(counter)를 증가한다.

키(key)로 문자, 계수기(counter)로 해당 값(value)을 갖는 딕셔너리를 생성한다. 
처음 문자를 만나면, 딕셔너리에 항목으로 추가한다.
추가한 후에는 기존 항목 값을 증가한다.

상기 3개 옵션은 동일한 연산을 수행하지만, 각각은 다른 방식으로 연산을 구현한다.

구현(implementation)은 연산(computation)을 수행하는 방법이다. 
어떤 구현 방법이 다른 것 보다 낫다.
예를 들어, 딕셔너리 구현의 장점은 사전에 문자열에서 어떤 문자가 나타날지 몰라도 된다. 다만 나타날 문자에 대한 공간만 준비하면 된다는 것이다.

다음에 딕셔너리로 구현한 코드가 있다.




계수기(counter) 혹은 빈도에 대한 통계 용어인 히스토그램(histogram)을 효과적으로 계산해보자.

for 루프는 문자열을 훑는다. 매번 루프를 반복할 때마다 딕셔너리에 문자 c가 없다면, 키 c와 초기값 1을 가진 새로운 항목을 생성한다.
문자 c가 이미 딕셔너리에 존재한다면, d[c]을 증가한다.

다음 프로그램 실행 결과가 있다.




히스토그램은 문자 'a', 'b'는 1회, 'o'는 2회 등등 나타남을 보여준다.

딕셔너리에는 키와 디폴트(default) 값을 갖는 get 메쏘드가 있다. 
딕셔너리에 키가 나타나면, get 메쏘드는 해당 값을 반환하고, 해당 값이 없으면 디폴트 값을 반환한다. 예를 들어,




get 메쏘드를 사용해서 상기 히스토그램 루프를 좀더 간결하게 작성할 수 있다.
get 메쏘드는 딕셔너리에 키가 존재하지 않는 경우를 자동적으로 다루기 때문에, if문을 없애 4줄을 1줄로 줄일 수 있다.




계수기(counter) 루프를 단순화하려고 get메쏘드를 사용하는 것은 파이썬에서 흔히 사용되는 "숙어(idiom)"가 되고, 책 후반부까지 많이 사용할 것이다.
시간을 가지고서 잠시 if 문과 in 연산자를 사용한 루프와 get메쏘드를 사용한 루프를 비교해 보세요.
동일한 연산을 수행하지만, 하나가 더 간결한다.


딕셔너리와 파일

딕셔너리의 흔한 사용법 중의 하나는 텍스트로 작성된 파일에 단어 빈도를 세는 것이다. 
http://shakespeare.mit.edu/Tragedy/romeoandjuliet/romeo_juliet.2.2.html 사이트 덕분에 
로미오와 쥴리엣(Romeo and Juliet) 텍스트 파일에서 시작합시다.

처음 연습으로 구두점이 없는 짧고 간략한 텍스트 버젼을 사용한다. 
나중에 구두점이 포함된 전체 텍스트로 작업할 것이다.




파일 라인을 읽고, 각 라인을 단어 리스트로 쪼개고, 루프를 돌려 사전을 이용하여 각 단어의 빈도수를 세는 파이썬 프로그램을 작성한다.

두 개의 for 루프를 사용한다. 
외곽 루프는 파일 라인을 읽고, 내부 루프는 특정 라인의 단어 각각에 대해 반복한다.
하나의 루프는 외곽 루프가 되고, 또 다른 루프는 내부 루프가 되어서 중첩 루프(nested loops)라고 불리는 패턴 사례다.

외곽 루프가 한번 반복을 할 때마다 내부 루프는 모든 반복을 수행하기 때문에 내부 루프는 "좀더 빨리" 반복을 수행하고 외곽 루프는 
좀더 천천히 반복을 수행하는 것으로 생각할 수 있다.

두 중첩 루프의 조합이 입력 파일의 모든 라인에 있는 모든 단어의 빈도를 계수(count)하도록 보증합니다.




프로그램을 실행하면, 정렬되지 않은 해쉬 순서로 모든 단어의 빈도수를 출력합니다.
romeo.txt 파일은 www.py4inf.com/code/romeo.txt에서 다운로드 가능하다.




가장 높은 빈도 단어와 빈도수를 찾기 위해서 딕셔너리를 훑는 것이 불편하다. 
좀더 도움이 되는 출력결과를 만들려고 파이썬 코드를 추가하자.


반복과 딕셔너리

for문에 순서(sequence)로서 딕셔너리를 사용한다면, 딕셔너리 키를 훑는다. 
루프는 각 키와 해당 값을 출력한다.




출력은 다음과 같다.




다시 한번, 키는 특별한 순서가 없다.

이 패턴을 사용해서 앞서 기술한 다양한 루프 숙어를 구현한다.
예를 들어, 딕셔너리에서 10 보다 큰 값을 가진 항목을 모두 찾고자 한다면, 다음과 같이 코드를 작성한다.




for 루프는 딕셔너리 키(keys)를 반복한다. 
그래서, 인덱스 연산자를 사용해서 각 키에 상응하는 값(value)을 가져와야 한다.
여기 출력값이 있다.




10 이상 값만 가진 항목만 볼 수 있다.

알파벳 순으로 키를 출력하고자 한다면, 딕셔너리 객체의 keys 메쏘드를 사용해서 딕셔너리 키 리스트를 생성한다.
그리고 나서 리스트를 정렬하고, 정렬된 리스트를 루프 돌리고, 아래와 같이 정렬된 순서로 키/값 페어(key/value pair)를 출력한다.




다음에 출력결과가 있다.




처음에 keys 메쏘드로부터 얻은 정렬되지 않은 키 리스트가 있고, 
그리고 나서 for 루프로 정렬된 키/값 페어(key/value pair)가 있다.


고급 텍스트 파싱

romeo.txt 파일을 사용한 상기 예제에서, 수작업으로 모든 구두점을 제거해서 가능한 단순하게 만들었다.
실제 텍스트는 아래 보여지는 것처럼 많은 구두점이 있다.




파이썬 split 함수는 공백을 찾고 공백으로 구분되는 토큰으로 단어를 처리하기 때문에, 
"soft!" 와 "soft"는 다른 단어로 처리되고 각 단어에 대해서 별도 딕셔너리 항목을 생성한다.

파일에 대문자가 있어서, "who"와 "Who"를 다른 단어, 다른 빈도수를 가진 것으로 처리한다.

lower, punctuation, translate 문자열 메쏘드를 사용해서 상기 문제를 해결할 수 있다.
translate 메쏘드가 가장 적합하다. 
translate 메쏘드에 대한 문서가 다음에 있다.

string.translate(s, table[, deletechars])

(만약 존재한다면) deletechars 에 있는 모든 문자를 삭제한다.
그리고 나서 순서수(ordinal)로 색인된 각 문자를 번역하는 256-문자열 테이블(table)을 사용해서 문자를 번역한다.
만약 테이블이 None 이면, 문자 삭제 단계만 수행된다. 

table을 명세하지는 않을 것이고, deletechars 매개변수를 사용해서 모든 구두점을 삭제할 것이다.
파이썬이 "구두점"으로 간주하는 문자 리스트를 출력하게 할 것이다.




프로그램을 다음과 같은 수정을 한다.




translate 메쏘드를 사용해서 모든 구두점을 제거했고, lower 메쏘드를 사용해서 라인을 소문자로 수정했다.
나머지 프로그램은 변경된게 없다. 
파이썬 2.5 이전 버젼에는 translate 메쏘드가 첫 매개변수로 None을 받지 않아서
translate 메쏘드를 호출하기 위해서 다음 코드를 사용하세요.




"파이썬 예술(Art of Python)" 혹은 "파이썬스럽게 생각하기(Thinking Pythonically)"를 배우는 일부분은
파이썬은 흔한 자료 분석 문제에 대해서 내장 기능을 가지고 있는 것을 깨닫는 것이다.
시간이 지남에 따라, 충분한 예제 코드를 보고 충분한 문서를 읽는다. 
작업을 편하게 할 수 있게 이미 다른 사람이 작성한 코드가 존재하는지를 살펴보기 위해서 어디를 찾아봐야 하는지도 알게 될 것이다.

다음은 출력결과의 축약 버젼이다.




출력결과는 여전히 다루기 힘들어 보입니다. 
파이썬을 사용해서 정확히 찾고자는 하는 것을 찾았으나 파이썬 튜플(tuples)에 대해서 학습할 필요성을 느껴진다.
튜플을 학습할 때, 다시 이 예제를 살펴볼 것이다.


디버깅
점점 더 큰 데이터로 작업함에 따라, 수작업으로 데이터를 확인하거나 출력을 통해서 디버깅을 하는 것이 어려울 수 있다.
큰 데이터를 디버깅하는 몇가지 방법이 있다.



[입력값을 줄여라(Scale down the input):]]
가능하면, 데이터 크기를 줄여라. 예를 들어, 프로그램이 텍스트 파일을 읽는다면, 첫 10줄로 시작하거나, 찾을 수 있는 작은 예제로 시작하라.
데이터 파일을 편집하거나, 프로그램을 수정해서 첫 n 라인만 읽도록 프로그램을 변경하라.

오류가 있다면, n을 줄여서 오류를 재현하는 가장 작은 값으로 만들어라.
그리고 나서, 오류를 찾고 수정해 나감에 따라 점진적으로 늘려나가라.

[요약값과 자료형을 확인하라(Check summaries and types):] 
전체 데이터를 출력하고 검증하는 대신에 데이터를 요약하여 출력하는 것을 생각하라: 예를 들어, 딕셔너리 항목의 숫자 혹은 리스트 숫자의 총계

실행 오류(runtime errors)의 일반적인 원인은 올바른 자료형(right type)이 아니기 때문이다. 
이런 종류의 오류를 디버깅하기 위해서, 종종 값의 자료형을 출력하는 것으로 충분하다.

[자가 진단 작성(Write self-checks):]

종종 오류를 자동적으로 검출하는 코드를 작성한다. 
예를 들어, 리스트 숫자의 평균을 계산한다면, 결과값은 리스트의 가장 큰 값보다 클 수 없고, 가장 작은 값보다 작을 수 없다는 것을 확인할 수 있다. 
"완전히 비상식적인" 결과를 탐지하기 때문에 "건전성 검사(sanity check)"라고 부른다.

또 다른 검사법은 두가지 다른 연산의 결과를 비교해서 일치하는지 살펴보는 것이다. 
"일치성 검사(consistency check)"라고 부른다.

[고급 출력(Pretty print the output):] 디버깅 출력결과를 서식화하는 것은 오류 발견을 용이하게 한다.

다시 한번, 발판(scaffolding)을 만드는데 들인 시간이 디버깅에 소비되는 시간을 줄일 수 있다.


용어정의



[딕셔너리(dictionary):] 키(key)에서 해당 값으로 매핑(mapping)
[해쉬테이블(hashtable):] 파이썬 딕셔너리를 구현하기 위해 사용된 알고리즘
[해쉬 함수(hash function):] 키에 대한 위치를 계산하기 위해서 해쉬테이블에서 사용되는 함수
[히스토그램(histogram):] 계수기(counter) 집합.
[구현(implementation):] 연산(computation)을 수행하는 방법
[항목(item):] 키-값 페어(key-value pair)에 대한 또 다른 이름.
[키(key):] 키-값 페어(key-value pair)의 첫번째 부분으로 딕셔너리에 나타나는 객체.
[키-값 페어(key-value pair):] 키에서 값으로 매핑 표현.
[룩업(lookup):] 키를 가지고 해당 값을 찾는 딕셔너리 연산.
[중첩 루프(nested loops):] 루프 "내부"에 하나 혹은 그 이상의 루프가 있음. 
외곽 루프가 1회 실행될 때, 내부 루프는 전체 반복을 완료함.
[값(value):] 키-값 페어(key-value pair)의 두번째 부분으로 딕셔너리에 나타나는 객체. 
앞에서 사용한 단어 "값(value)" 보다 더 구체적이다.

연습문제



커밋(commit)이 무슨 요일에 수행되었는지에 따라 전자우편 메세지를 구분하는 프로그램을 작성하세요.
"From"으로 시작하는 라인을 찾고, 3번째 단어를 찾아서 요일별 횟수를 계수(count)하여 저장하세요.
프로그램 끝에 딕셔너리 내용을 출력하세요. (순서는 문제가 되지 않습니다.)



전자우편 로그(log)를 읽고, 히스토그램을 생성하는 프로그램을 작성하세요.
딕셔너리를 사용해서 전자우편 주소별로 얼마나 많은 전자우편이 왔는지를 계수(count)하고 딕셔너리를 출력합니다.




상기 프로그램에 누가 가장 많은 전자우편 메시지를 가졌는지 알아내는 코드를 추가하세요.

결국, 모든 데이터를 읽고, 딕셔너리를 생성한다. 
최대 루프(장  참조)를 사용해서 딕셔너리를 훑어서 누가 가장 많은 전자우편 메시지를 갖는지, 
그리고 그 사람이 얼마나 많은 메시지를 갖는지 출력한다.






다음 프로그램은 주소 대신에 도메인 이름을 기록한다. 
누가 메일을 보냈는지 대신(즉, 전체 전자우편 주소) 메시지가 어디에서부터 왔는지 출처를 기록한다.
프로그램 마지막에 딕셔너리 내용을 출력한다.






튜플(Tuples)

튜플은 불변이다.

튜플(tuple)(재미난 사실: 단어 "튜플(tuple)"은 가변 길이 (한배, 두배, 세배, 네배, 다섯배, 여섯배, 일곱배 등) 숫자열에 
붙여진 이름에서 유래한다.)은 리스트와 마찬가지로 순서(sequence) 값이다. 
튜플에 저장된 값은 임의 자료형(type)이 될 수 있고, 정수로 색인 된다.
중요한 차이점은 튜플은 불변(immutable)하다는 것이다.
튜플은 또한 비교 가능(comparable)하고 해쉬형(hashable)이다.
따라서, 리스트 값을 정렬할 수 있고, 파이썬 딕셔너리 키 값으로 튜플을 사용할 수 있다.

구문론적으로, 튜플은 콤마로 구분되는 리스트 값이다.




꼭 필요하지는 않지만, 파이썬 코드를 봤을 때, 가독성을 높여 튜플을 빠르게 알아볼 수 있도록 괄호로 튜플을 감싸는 것이 일반적이다.




단일 요소를 가진 튜플을 생성하기 위해서 마지막 콤마를 포함해야 한다.




콤마가 없는 경우 파이썬에서는 ('a')을 괄호를 가진 문자열 표현으로 간주하여 문자열로 평가한다.




튜플을 구축하는 다른 방법은 내장함수 tuple을 사용하는 것이다. 
인자가 없는 경우, 빈 튜플을 생성한다.




만약 인자가 문자열, 리스트 혹은 튜플 같은 순서(sequence)인 경우, 
tuple에 호출한 결과는 요소 순서(sequence)를 가진 튜플이 된다.




튜플(tuple)이 생성자 이름이기 때문에 변수명으로 튜플 사용을 피해야 한다.

대부분의 리스트 연산자는 튜플에서도 사용 가능하다. 
꺾쇠 연산자가 요소를 색인한다.




그리고, 슬라이스 연산자(slice operator)는 요소 범위를 선택한다.




하지만, 튜플 요소 중 하나를 변경하고 하면, 오류가 발생한다.




튜플 요소를 변경할 수는 없지만, 튜플을 다른 튜플로 교체는 할 수 있다.





튜플 비교하기

비교 연산자는 튜플과 다른 순서(sequence)와 함께 쓸 수 있다. 
파이썬은 각 순서(sequence)에서 비교를 첫 요소부터 시작한다.
만약 두 요소가 같다면, 다음 요소 비교를 진행하며 서로 다른 요소를 찾을 때까지 계속한다. 
후속 요소가 아무리 큰 값이라고 하더라도 비교 고려대상에서 제외된다.




정렬 (sort) 함수도 동일한 방식으로 동작한다.
첫 요소를 먼저 정렬하지만, 동일한 경우 두 번째 요소를 정렬하고, 그 후속 요소를 동일한 방식으로 정렬한다.

이 기능이 다음 DSU라고 불리는 패턴이 된다.



[데코레이트 (Decorate)] : 순서(sequence)에서 요소 앞에 하나 혹은 그 이상의 키를 가진 튜플 리스트를 구축해서 순서(sequence)를 장식한다.

[정렬 (Sort)] : 파이썬 내장 함수 sort를 사용한 튜플 리스트를 정렬한다.

[언데코레이트 (Undecorate)] : 순서(sequence)의 정렬된 요소만 추출하여 장식을 지웁니다.

예를 들어, 단어 리스트가 있고 가장 긴 단어부터 가장 짧은 단어 순으로 정렬한다고 가정하자.




첫번째 루프는 튜플 리스트를 생성하고, 각 튜플은 단어 앞에 길이 정보를 가진다.

정렬(sort)함수는 첫번째 요소, 길이를 우선 비교하고, 동률일 경우에만 두 번째 요소를 고려한다.
정렬(sort) 함수의 인자 reverse=True는 내림차순으로 정렬한다는 의미다.

두 번째 루프는 튜플 리스트를 운행하여 훑고, 길이에 따라 내림차순으로 단어 리스트를 생성한다.
그래서, 5 문자 단어는 역 알파벳 순으로 정렬되어 있다. 
다음 리스트에서 "what"이 "soft" 보다 앞에 나타난다.

프로그램의 출력은 다음과 같다.





물론, 파이썬 리스트로 변환하여 내림차순으로 정렬된 문장은 시적인 의미를 많이 잃어버렸다.


 튜플 대입(Tuple Assignment)
파이썬 언어의 독특한 구문론적인 기능중의 하나는 대입문의 왼편에 튜플을 놓을 수 있다는 것이다. 
왼편이 순서(sequence)인 경우 한번에 하나 이상의 변수에 대입할 수 있다.

다음 예제에서, 순서(sequence)인 두개 요소를 갖는 리스트가 있다. 
하나의 문장으로 순서(sequence)의 첫번째와 두번째 요소를 변수 x와 y에 대입한다. 




마술이 아니다. 파이썬은 대략 튜플 대입 구문을 다음과 같이 해석한다.(파이썬은 구문을 문자 그대로 해석하지는 않는다.
예를 들어, 동일한 것을 딕셔너리로 작성한다면, 기대한 것처럼 동작하지는 않는다.)


스타일적으로 대입문 왼편에 튜플을 사용할 때, 괄호를 생략한다. 
하지만 다음은 동일하게 적합한 구문이다.




튜플 대입문을 사용하는 특히 똑똑한 응용사례는 단일 문장으로 두 변수 값을 교환(swap)하는 것이다.




양쪽 문장이 모두 튜플이지만, 왼편은 튜플 변수이고 오른편은 튜플 표현식이다.
오른편 각각의 값이 왼편 해당 변수에 대입된다. 
대입이 이루어지기 전에 오른편의 모든 표현식이 평가된다.

왼편의 변수 갯수와 오른편의 값의 갯수는 동일해야 한다.




좀더 일반적으로, 오른편은 임의 순서(문자열, 리스트 혹은 튜플)가 될 수 있다.
예를 들어, 전자우편 주소를 사용자 이름과 도메인으로 분할하기 위해서 다음과 같이 프로그램을 작성할 수 있다.




분할 (split) 함수로부터 반환되는 값은 두개 요소를 가진 리스트다. 
첫번째 요소는 uname에 두번째 요소는 domain에 대입된다.





딕셔너리와 튜플

딕셔너리에는 튜플 리스트를 반환하는 items 메쏘드가 있다. 
각 튜플은 키-값 페어(key-value pair)다.(파이썬 3.0으로 가면서 살짝 달라졌다.)




딕셔너리로부터 기대했듯이, 항목은 특별한 순서가 없다.

하지만 튜플 리스트는 리스트여서 비교가 가능하기 때문에, 튜플 리스트를 정렬할 수 있다.
딕셔너리를 튜플 리스트로 변환하는 것이 키로 정렬된 딕셔너리 내용을 출력할 수 있게 한다.




새로운 리스트는 키 값으로 오름차순 알파벳 순으로 정렬된다.


딕셔너리로 다중 대입

items 함수, 튜플 대입, for문을 조합해서, 
단일 루프로 딕셔너리의 키와 값을 운행하여 훑는 멋진 코드 패턴을 만들 수 있다.




상기 루프에는 두개의 반복 변수(iteration variables)가 있다.
items 함수가 튜플 리스트를 반환하고, key, val는 튜플 대입하여 딕셔너리에 있는 각각의 키-값 페어(key-value pair)를 성공적으로 반복한다.

매번 루프를 반복할 때마다, key와 value는 딕셔너리(여전히 해쉬 순으로 되어 있음)의 다음 키-값 페어(key-value pair)로 진행한다.

루프의 출력결과는 다음과 같다.




다시 한번 해쉬 키 순서다. (즉, 특별한 순서가 없다.)

두 기술을 조합하면, 딕셔너리 내용을 키-값 페어(key-value pair)에 저장된 값의 순서로 정렬하여 출력할 수 있다.

이것을 수행하기 위해서, 각 튜플이 (value, key) 형태인 튜플 리스트를 작성한다. 
items 메쏘드를 사용하여 리스트 (key, value) 튜플을 만든다. 
하지만 이번에는 키가 아닌 값으로 정렬한다.
키-값(key-value) 튜플 리스트를 생성하면, 역순으로 리스트를 정렬하고 새로운 정렬 리스트를 출력하는 것은 쉽다.




조심스럽게 각 튜플 첫번째 요소로 값(value)을 갖는 튜플 리스트를 생성했다. 
튜플 리스트를 정렬하여 값으로 정렬된 딕셔너리가 생성되었다.


가장 빈도수가 높은 단어

로미오와 쥴리엣 2장 2막 텍스트 파일로 다시 돌아와서, 텍스트에서 가장 빈도수가 높은 단어를 10개를 출력하기 위해서
상기 학습한 기법을 사용하여 프로그램을 보강해보자.




파일을 읽고 각 단어를 문서의 단어 빈도수에 매핑(사상)하여 딕셔너리를 계산하는 프로그램 첫 부분은 바뀌지 않는다.
하지만, counts 를 단순히 출력하는 대신에 (val, key) 튜플 리스트를 생성하고 역순으로 리스트를 정렬한다.

값이 첫 위치에 있기 때문에, 비교 목적으로 값을 사용한다. 
만약 동일한 값을 가진 튜플이 하나이상 존재한다면, 두번째 요소(키, key)를 살펴본다.
그래서 값이 동일한 경우 키의 알파벳 순으로 추가 정렬된다.

마지막에 다중 대입 반복을 수행하는 멋진 for 루프를 작성한다. 
그리고, 리스트 슬라이스(lst[:10])를 통해 가장 빈도수가 높은 상위 10개 단어를 출력한다.

이제 단어 빈도 분석을 위해서 작성한 프로그램의 마지막 출력결과는 원하는 바를 완수한 것처럼 보인다.




복잡한 데이터 파싱과 분석 작업이 이해하기 쉬운 19줄 파이썬 프로그램으로 수행된 사실이 왜 파이썬이 정보 탐색 언어로서 좋은 선택인지 보여준다.


 딕셔너리 키로 튜플 사용하기

튜플은 해쉬형(hashable)이고, 리스트는 그렇지 못하기 때문에, 만약 딕셔너리에 사용할 복합(composite)키를 생성하려면, 키로 튜플을 사용해야 한다.

만약 성(last-name)과 이름(first-name)을 가지고 전화번호에 사상(매핑, mapping)하는 전화번호부를 생성하려고 하면, 복합키와 마주친다.
변수 last, first, number을 정의했다고 가정하면, 다음과 같이 딕셔너리 대입문을 작성할 수 있다.




꺾쇠 괄호 표현은 튜플이다. 
딕셔너리를 훑기 위해서 for 루프에 튜플 대입을 사용한다.




상기 루프가 튜플인 directory에 키를 훑는다. 
각 튜플 요소를 last, first에 대입하고 나서, 이름과 해당 전화번호를 출력한다.


순서(sequence) : 문자열, 리스트, 튜플
여기서 리스트 튜플에 초점을 맞추었지만, 이장의 거의 모든 예제가 또한 리스트의 리스트, 튜플의 튜플, 리스트 튜플에도 동작한다.
가능한 모든 조합을 열거하는 것을 피하기 위해서, 순서의 순서(sequences of sequences)에 대해서 논의하는 것이 때로는 쉽다.

대부분의 문맥에서 다른 종류의 순서(문자열, 리스트, 튜플)는 상호 호환해서 사용될 수 있다. 
그런데 왜 그리고 어떻게 다른 것보다 이것을 선택해야 될까?

명확한 것부터 시작하자.
문자열은 요소가 문자여야 하기 때문에 다른 순서(sequence)보다 제약이 따른다.
또한 문자열은 불변(immutable)이다. 
새로운 문자열을 생성하는 것과 반대로, 문자열에 있는 문자를 변경하고자 한다면, 
대신에 문자 리스트를 사용하는 것을 생각할 수 있다.

리스트는 튜플보다 좀더 일반적으로 사용된다. 
이유는 대체로 변경가능(mutable)하기 때문이다.
하지만, 다음 몇가지 경우에 튜플이 좀더 선호된다.



return 문처럼 어떤 맥락에서, 리스트보다 튜플을 생성하는 것이 구문론적으로 더 간략하다.
다른 맥락에서는 리스트가 더 선호될 수 있다.

딕셔너리 키로서 순서(sequence)를 사용하려면, 튜플이나 문자열같은 불변 자료형(immutable type)을 사용해야 한다.

함수에 인자로 순서(sequence)를 전달하려면, 튜플을 사용하는 것이 에일리어싱(aliasing)으로 생기는 예기치 못한 행동에 대한 가능성을 줄여 준다.

튜플은 불변(immutable)이어서, 기존 리스트를 변경하는 sort, reverse 같은 메쏘드를 제공하지는 않는다.
하지만, 파이썬이 제공하는 내장함수 sorted, reversed 를 통해서, 매개 변수로 임의 순서(sequence)를 전달 받아서, 같은 요소를 다른 순서로 정렬된 새로운 리스트를 반환한다.


디버깅

리스트, 딕셔너리, 튜플은 자료 구조(data structures)로 일반적으로 알려져 있다.
이번장에서 리스트 튜플, 키로 튜플, 값으로 리스트를 담고 있는 딕셔너리 같은 복합 자료 구조를 보기 시작했다.
복합 자료 구조는 유용하지만, 저자가 작명한 모양 오류(shape errors)라고 불리는 오류에 노출되어 있다.
즉, 자료 구조가 잘못된 자료형(type), 크기, 구성일 경우 오류가 발생한다. 
혹은 코드를 작성하고, 자료의 모양이 생각나지 않는 경우도 오류의 원인이 된다.

예를 들어, 정수 하나를 가진 리스트를 기대하고, (리스트가 아닌) 일반 정수를 넘긴다면, 작동하지 않는다.

프로그램을 디버깅할 때, 정말 어려운 버그를 잡으려고 작업을 한다면, 다음 네가지를 시도할 수 있다.



[코드 읽기(reading):] 
코드를 면밀히 조사하고, 스스로에게 다시 읽어 주고, 코드가 자신이 작성한 의도를 담고 있는지  점검하라.

[실행(running):] 
변경해서 다른 버젼을 실행해서 실험하라. 
종종, 프로그램이 적절한 곳에 적절한 것을 보여준다면, 문제가 명확하다. 
발판(scaffolding)을 만들기 위해서 때때로 시간을 들일 필요도 있다.

[반추(ruminating):] 
생각의 시간을 갖자. 
어떤 종류의 오류인가: 구문, 실행, 의미론(semantic). 
오류 메시지로부터 혹은 프로그램 출력결과로부터 무슨 정보를 얻을 수 있는가?
어떤 종류 오류가 지금 보고 있는 문제를 만들었을까? 
문제가 나타나기 전에, 마지막으로 변경한 것은 무엇인가?

[퇴각(retreating):]
어느 시점에선가, 최선은 물러서서, 최근의 변경을 다시 원복하는 것이다. 
잘 동작하고 이해하는 프로그램으로 다시 돌아가서, 다시 프로그램을 작성한다.
초보 프로그래머는 종종 이들 활동 중 하나에 사로잡혀 다른 것을 잊곤 한다. 
활동 각각은 고유한 실패 방식과 함께 온다.

예를 들어, 프로그램을 정독하는 것은 문제가 인쇄상의 오류에 있다면 도움이 되지만, 문제가 개념상 오해에 뿌리를 두고 있다면 그다지 도움이 되지 못한다. 
만약 작성한 프로그램을 이해하지 못한다면, 100번 읽을 수는 있지만, 오류를 발견할 수는 없다.
왜냐하면, 오류는 여러분 머리에 있기 때문입니다.

만약 작고 간단한 테스트를 진행한다면, 실험을 수행하는 것이 도움이 될 수 있다.
하지만, 코드를 읽지 않거나, 생각없이 실험을 수행한다면, 프로그램이 작동될 때까지 무작위 변경하여 개발하는 "랜덤 워크 프로그램(random walk programming)" 패턴에 빠질 수 있다. 
말할 필요없이 랜덤 워크 프로그래밍은 시간이 오래 걸린다.

생각할 시간을 가져야 한다. 
디버깅은 실험 과학 같은 것이다. 
문제가 무엇인지에 대한 최소한 한 가지 가설을 가져야 한다.
만약 두개 혹은 그 이상의 가능성이 있다면, 이러한 가능성 중에서 하나라도 줄일 수 있는 테스트를 생각해야 한다.

휴식 시간을 가지는 것은 생각하는데 도움이 된다. 
대화를 하는 것도 도움이 된다.
문제를 다른 사람 혹은 자신에게도 설명할 수 있다면, 질문을 마치기도 전에 답을 종종 발견할 수 있다.

하지만, 오류가 너무 많고 수정하려는 코드가 매우 크고, 복잡하다면 최고의 디버깅 기술도 무용지물이다.
가끔, 최선의 선택은 퇴각하는 것이다. 
작동하고 이해하는 곳까지 후퇴해서 프로그램을 간략화하라.

초보 프로그래머는 종종 퇴각하기를 꺼려한다. 
왜냐하면, 설사 잘못되었지만, 한줄 코드를 지울 수 없기 때문이다.
삭제하지 않는 것이 기분을 좋게 한다면, 다시 작성하기 전에 프로그램을 다른 파일에 복사하라.
그리고 나서, 한번에 조금씩 붙여넣어라. 

정말 어려운 버그(hard bug)를 발견하고 고치는 것은 코드 읽기, 실행, 반추, 때때로 퇴각을 요구한다.
만약 이들 활동 중 하나도 먹히지 않는다면, 다른 것들을 시도해 보세요.


용어정의



[비교가능한(comparable):] 동일한 자료형의 다른 값과 비교하여 큰지, 작은지, 혹은 같은지를 확인하기 위해서 확인할 수 있는 자료형(type).
비교가능한(comparable) 자료형은 리스트에 넣어서 정렬할 수 있다.
[자료 구조(data structure):] 연관된 값의 집합, 종종 리스트, 딕셔너리, 튜플 등으로 조직화된다.
[DSU:] "decorate-sort-undecorate,"의 약어로 리스트 튜플을 생성, 정렬, 결과 일부 추출을 포함하는 패턴.
[모음(gather):] 가변-길이 인자 튜플을 조합하는 연산.
[해쉬형(hashable):] 해쉬 함수를 가진 자료형(type). 
정수, 소수점, 문자열 같은 불변형은 해쉬형이다. 
리스트나 딕셔너리 처럼 변경가능한 형은 해쉬형이 아니다.
[스캐터(scatter):] 순서(sequence)를 리스트 인자로 다루는 연산.
[(자료 구조의) 모양 (shape (of a data structure)):] 자료 구조의 자료형(type), 크기, 구성을 요약.
[싱글톤(singleton):] 단일 요소를 가진 리스트 (혹은 다른 순서(sequence)).
[튜플(tuple):] 불변 요소들의 순서 (sequence).
[튜플 대입(tuple assignment):] 오른편 순서(sequence)와 왼편 튜플 변수를 대입.
오른편이 평가되고나서 각 요소들은 왼편의 변수에 대입된다.

연습문제


앞서 작성한 프로그램을 다음과 같이 수정하세요.
"From"라인을 읽고 파싱하여 라인에서 주소를 뽑아내세요.
딕셔너리를 사용하여 각 사람으로부터 메시지 숫자를 계수(count)한다.

모든 데이터를 읽은 후에 가장 많은 커밋(commit)을 한 사람을 출력하세요.
딕셔너리로부터 리스트 (count, email) 튜플을 생성하고 역순으로 리스트를 정렬한 후에 가장 많은 커밋을 한 사람을 출력하세요.




이번 프로그램은 각 메시지에 대한 하루 중 시간의 분포를 계수(count)한다.
"From" 라인으로부터 시간 문자열을 찾고 콜론(:) 문자를 사용하여 문자열을 쪼개서 시간을 추출합니다.
각 시간별로 계수(count)를 누적하고 아래에 보여지듯이 시간 단위로 정렬하여 한 라인에 한시간씩 계수(count)를 출력합니다.



파일을 읽고, 빈도(frequencey)에 따라 내림차순으로 문자(letters)를 출력하는 프로그램을 작성하세요.
작성한 프로그램은 모든 입력을 소문자로 변환하고 a-z 문자만 계수(count)한다. 
공백, 숫자, 문장기호 a-z를 제외한 다른 어떤 것도 계수하지 않습니다.
다른 언어로 구성된 텍스트 샘플을 구해서 언어마다 문자 빈도가 어떻게 변하는지 살펴보세요.
결과를 wikipedia.org/wiki/Letter_frequencies 표와 비교하세요.






정규 표현식

지금까지 파일을 훑어서 패턴을 찾고, 관심있는 라인에서 다양한 비트(bits)를 뽑아냈다. 
split, find 같은 문자열 메쏘드를 사용하였고, 라인에서 일정 부분을 뽑아내기 위해서 리스트와 문자열 슬라이싱(slicing)을 사용했다.

검색하고 추출하는 작업은 너무 자주 있는 일이어서 파이썬은 상기와 같은 작업을 매우 우아하게 처리하는 정규 표현식(regular expressions)으로 불리는 매우 강력한 라이브러리를 제공한다.
정규 표현식을 책의 앞부분에 소개하지 않은 이유는 정규 표현식 라이브러리가 매우 강력하지만, 약간 복잡하고, 구문에 익숙해지는데 시간이 필요하기 때문이다.

정규표현식은 문자열을 검색하고 파싱하는데 그 자체가 작은 프로그래밍 언어다.
사실, 책 전체가 정규 표현식을 주제로 쓰여진 책이 몇권 있다.
이번 장에서는 정규 표현식의 기초만을 다룰 것이다.
정규 표현식의 좀더 자세한 사항은 다음을 참조하라.

http://en.wikipedia.org/wiki/Regular_expression

http://docs.python.org/library/re.html

정규 표현식 라이브러리는 사용하기 전에 프로그램을 가져오기(import)해야 한다.
정규 표현식 라이브러리의 가장 간단한 쓰임은  search() 검색 함수다. 
다음 프로그램은 검색 함수의 사소한 사용례를 보여준다.




파일을 열고, 각 라인을 루프로 반복해서 정규 표현식 search() 메쏘드를 호출하여 문자열 "From"이 포함된 라인만 출력한다.
상기 프로그램은 진정으로 강력한 정규 표현식 기능을 사용하지 않았다. 
왜냐하면, line.find() 메쏘드를 가지고 동일한 결과를 쉽게 구현할 수 있기 때문이다.

정규 표현식의 강력한 기능은 문자열에 해당하는 라인을 좀더 정확하게 제어하기 위해서 검색 문자열에 특수문자를 추가할 때 확인될 수 있다.
매우 적은 코드를 작성할지라도, 정규 표현식에 특수 문자를 추가하는 것만으로도 정교한 일치(matching)와 추출이 가능하게 한다.

예를 들어, 탈자 기호(caret)는 라인의 "시작"과 일치하는 정규 표현식에 사용된다.
다음과 같이 "From:"으로 시작하는 라인만 일치하도록 응용프로그램을 변경할 수 있다.




"From:" 문자열로 시작하는 라인만 일치할 수 있다. 
여전히 매우 간단한 프로그램으로 문자열 라이브러리에서  startswith() 메쏘드로 동일하게 수행할 수 있다.
하지만, 무엇을 정규 표현식과 매칭하는가에 대해 특수 액션 문자를 담아 좀더 많은 제어를 할수 있게 하는 정규 표현식 개념을 소개하기에는 충분하다.


정규 표현식의 문자 매칭

좀더 강력한 정규 표현식을 작성할 수 있는 다른 특수문자는 많이 있다.
가장 자주 사용되는 특수 문자는 임의 문자를 매칭하는 마침표다.

다음 예제에서 정규 표현식 "F..m:"은 "From:", "Fxxm:", "F12m:", "F!@m:' 같은 임의 문자열을 매칭한다. 
왜냐하면 정규 표현식 마침표 문자가 임의의 문자와 매칭되기 때문이다.




정규 표현식에 "*", "+' ' 문자를 사용하여 문자가 원하는만큼 반복을 나타내는 기능과 결합되었을 때는 더욱 강력해진다.
"*", "+' ' 특수 문자가 검색 문자열에 문자 하나만을 매칭하는 대신에 별표 기호인 경우 0 혹은 그 이상의 매칭, 더하기 기호인 경우 1 혹은 그 이상의 문자의 매칭을 의미한다.

다음 예제에서 반복 와일드 카드(wild card) 문자를 사용하여 매칭하는 라인을 좀더 좁힐 수 있다.




검색 문자열 "^From:.+@" 은 "From:" 으로 시작하고, ".+" 하나 혹은 그 이상의 문자들, 그리고 @ 기호와 매칭되는 라인을 성공적으로 찾아낸다.
그래서 다음 라인은 매칭이 될 것이다.


From: stephen.marquard@uct.ac.za
콜론(:)과  @ 기호 사이의 모든 문자들을 매칭하도록 확장하는 것으로 ".+" 와이드 카드를 간주할 수 있다.


From:.+@
더하기와 별표 기호를 "밀어내기(pushy)" 문자로 생각하는 것이 좋다. 
예를 들어, 다음 문자열은 ".+" 특수문자가 다음에 보여주듯이 밖으로 밀어내는 것처럼 문자열 마지막 @ 기호를 매칭한다.


From: stephen.marquard@uct.ac.za, csev@umich.edu, and cwen@iupui.edu
다른 특수문자를 추가함으로서 별표나 더하기 기호가 너무 "탐욕(greedy)"스럽지 않게 만들 수 있다.
와일드 카드 특수문자의 탐욕스러운 기능을 끄는 것에 대해서는 자세한 정보를 참조바란다. 


정규 표현식 사용 데이터 추출

파이썬으로 문자열에서 데이터를 추출하려면, findall() 메쏘드를 사용해서 정규 표현식과 매칭되는 모든 부속 문자열을 추출할 수 있다.
형식에 관계없이 임의 라인에서 전자우편 주소 같은 문자열을 추출하는 예제를 사용하자.
예를 들어, 다음 각 라인에서 전자우편 주소를 뽑아내고자 한다.




각각의 라인에 대해서 다르게 쪼개고, 슬라이싱하면서 라인 각각의 형식에 맞추어 코드를 작성하고는 싶지는 않다.
다음 프로그램은 findall() 메쏘드를 사용하여 전자우편 주소가 있는 라인을 찾아내고 하나 혹은 그 이상의 주소를 뽑아낸다.




findall() 메쏘드는 두번째 인자 문자열을 찾아서 전자우편 주소처럼 보이는 모든 문자열을 리스트로 반환한다.
공백이 아닌 문자 (S)와 매칭되는 두 문자 순서(sequence)를 사용한다.

프로그램의 출력은 다음과 같다.




정규 표현식을 해석하면, 적어도 하나의 공백이 아닌 문자, @과 적어도 하나 이상의 공백이 아닌 문자를 가진 부속 문자열을 찾는다.
또한, "S+" 특수 문자는 가능한 많이 공백이 아닌 문자를 매칭한다. (정규 표현식에서  "탐욕(greedy)" 매칭이라고 부른다.)

정규 표현식은 두번 매칭(csev@umich.edu, cwen@iupui.edu)하지만, 문자열 "@2PM"은 매칭을 하지 않는다.
왜냐하면, @ 기호 앞에 공백이 아닌 문자가 하나도 없기 때문이다.
프로그램의 정규 표현식을 사용해서 파일에 모든 라인을 읽고 다음과 같이 전자우편 주소처럼 보이는 모든 문자열을 출력한다.



각 라인을 읽어 들이고, 정규 표현식과 매칭되는 모든 부속 문자열을 추출한다. 
findall() 메쏘드는 리스트를 반환하기 때문에, 전자우편 처럼 보이는 부속 문자열을 적어도 하나 찾아서 출력하기 위해서 
반환 리스트 요소 숫자가 0 보다 큰지 여부를 간단히 확인한다.

mbox.txt 파일에 프로그램을 실행하면, 다음 출력을 얻는다.




전자우편 주소 몇몇은 "<", ";" 같은 잘못된 문자가 앞과 뒤에 붙어있다.
문자나 숫자로 시작하고 끝나는 문자열 부분만 관심있다고 하자.

그러기 위해서, 정규 표현식의 또 다른 기능을 사용한다. 
매칭하려는 다중 허용 문자 집합을 표기하기 위해서 꺾쇠 괄호를 사용한다. 
그런 의미에서 "S"은 공백이 아닌 문자 집합을 매칭하게 한다. 
이제 매칭하려는 문자에 관해서 좀더 명확해졌다.

여기 새로운 정규 표현식이 있다.




약간 복잡해졌다. 왜 정규 표현식이 자신만의 언어인가에 대해서 이해할 수 있다. 
이 정규 표현식을 해석하면, 
0 혹은 그 이상의 공백이 아닌 문자("S*")로 하나의 소문자, 대문자 혹은 숫자("[a-zA-Z0-9]")를 가지며, @ 다음에 0 혹은 그 이상의 공백이 아닌 문자("S*")로 하나의 소문자, 대문자 혹은 숫자("[a-zA-Z0-9]")로 된 부속 문자열을 찾는다.
0 혹은 그 이상의 공백이 아닌 문자를 나타내기 위해서 "+"에서 "*"으로 바꿨다. 
왜냐하면 "[a-zA-Z0-9]" 자체가 이미 하나의 공백이 아닌 문자이기 때문이다.
"*", "+"는 단일 문자에 별표, 더하기 기호 왼편에 즉시 적용됨을 기억하세요.

프로그램에 정규 표현식을 사용하면, 데이터가 훨씬 깔끔해진다.







"source@collab.sakaiproject.org" 라인에서 문자열 끝에 ">;" 문자를 정규 표현식으로 제거한 것을 주목하세요.
정규 표현식 끝에 "[a-zA-Z]"을 추가하여서 정규 표현식 파서가 찾는 임의 문자열은 문자로만 끝나야 되기 때문이다.
그래서, "sakaiproject.org>;"에서 ">"을 봤을 때, "g"가 마지막 맞는 매칭이 되고, 거기서 마지막 매칭을 마치고 중단한다.

프로그램의 출력은 리스트의 단일 요소를 가진 문자열로 파이썬 리스트이다.


검색과 추출 조합하기

다음과 같은 "X-" 문자열로 시작하는 라인의 숫자를 찾고자 한다면,




임의의 라인에서 임의 부동 소수점 숫자가 아니라 상기 구문을 가진 라인에서만 숫자를 추출하고자 한다.

라인을 선택하기 위해서 다음과 같이 정규 표현식을 구성한다.




정규 표현식을 해석하면, ''^''에서 "X-"으로 시작하고, ".*"에서 0 혹은 그이상의 문자를 가지며, 콜론(":")이 나오고 나서 공백을 만족하는 라인을 찾는다.
공백 뒤에 "[0-9.]+"에서 숫자 (0-9) 혹은 점을 가진 하나 혹은 그 이상의 문자가 있여야 한다.
꺽쇠 기호 사이에 마침표는 실제 마침표만 매칭함을 주목하기 바란다. (즉, 꺾쇠 기호 사이는 와일드 카드 문자가 아니다.)

관심을 가지고 있는 특정한 라인과 매우 정확하게 매칭이되는 매우 빠듯한 정규 표현식으로 다음과 같다.




프로그램을 실행하면, 잘 걸러져서 찾고자 하는 라인만 볼 수 있다.




하지만, 이제 rsplit 사용해서 숫자를 뽑아내는 문제를 해결해야 한다.
rsplit을 사용하는 것이 간단해 보이지만, 동시에 라인을 검색하고 파싱하기 위해서 정규 표현식의 또 다른 기능을 사용할 수 있다.

괄호는 정규 표현식의 또 다른 특수 문자다. 
정규 표현식에 괄호를 추가한다면, 문자열이 매칭될 때, 무시된다.
하지만, findall()을 사용할 때, 매칭할 전체 정규 표현식을 원할지라도, 정규 표현식을 매칭하는 부속 문자열의 부분만을 뽑아낸다는 것을 괄호가 표시한다.

그래서, 프로그램을 다음과 같이 수정한다.




search()을 호출하는 대신에, 매칭 문자열의 부동 소수점 숫자만 뽑아내는 findall()에 원하는 부동 소수점 숫자를 표현하는 정규 표현식 부분에 괄호를 추가한다.

프로그램의 출력은 다음과 같다.




숫자가 여전히 리스트에 있어서 문자열에서 부동 소수점으로 변환할 필요가 있지만, 흥미로운 정보를 찾아 뽑아내기 위해서 정규 표현식의 강력한 힘을 사용했다.

이 기술을 활용한 또 다른 예제로, 파일을 살펴보면, 폼(form)을 가진 라인이 많다.




상기 언급한 동일한 기법을 사용하여 모든 변경 번호(라인의 끝에 정수 숫자)를 추출하고자 한다만, 다음과 같이 프로그램을 작성할 수 있다.




작성한 정규 표현식을 해석하면, "Details:"로 시작하는 ".*"에 임의의 문자들로, "rev="을 포함하고 나서, 하나 혹은 그 이상의 숫자를 가진 라인을 찾는다.
전체 정규 표현식을 만족하는 라인을 찾고자 하지만, 라인 끝에 정수만을 추출하기 위해서 "[0-9]+"을 괄호로 감쌌다.

프로그램을 실행하면, 다음 출력을 얻는다.




"[0-9]+"은 "탐욕(greedy)"스러워서, 숫자를 추출하기 전에 가능한 큰 문자열 숫자를 만들려고 한다는 것을 기억하라.
이런 "탐욕(greedy)"스러운 행동으로 인해서 왜 각 숫자로 모두 5자리 숫자를 얻은 이유가 된다.
정규 표현식 라이브러리는 양방향으로 파일 처음이나 끝에 숫자가 아닌 것을 마주칠 때까지 뻗어 나간다.

이제 정규 표현식을 사용해서 각 전자우편 메시지의 요일에 관심이 있었던 책 앞의 연습 프로그램을 다시 작성한다.
다음 형식의 라인을 찾는다.




그리고 나서, 각 라인의 요일의 시간을 추출하고자 한다. 
앞에서 split를 두번 호출하여 작업을 수행했다. 첫번째는 라인을 단어로 쪼개고, 다섯번째 단어를 뽑아내서,
관심있는 두 문자를 뽑아내기 위해서 콜론 문자에서 다시 쪼갰다.



작동을 할지 모르지만, 실질적으로 정말 부서지기 쉬운 코드로 라인이 잘 짜여져 있다고 가정하에 가능하다. 
잘못된 형식의 라인이 나타날 때도 결코 망가지지 않는 프로그램을 담보하기 위해서 충분한 오류 검사기능을 추가하거나 커다란 try/except 블록을 넣으면, 참 읽기 힘든 10-15 라인 코드로 커질 것이다.

다음 정규 표현식으로 훨씬 간결하게 작성할 수 있다.




상기 정규 표현식을 해석하면, 공백을 포함한 "From "으로 시작해서, ".*"에 임의 갯수의 문자, 그리고 공백, 두 개의 숫자 "[0-9][0-9]" 뒤에 콜론(:) 문자를 가진 라인을 찾는다. 
일종의 찾고 있는 라인에 대한 정의다. 

findall()을 사용해서 단지 시간만 뽑아내기 위해서, 두 숫자에 괄호를 다음과 같이 추가한다.




작업 결과는 다음과 같이 프로그램에 반영한다.




프로그램을 실행하면, 다음 출력 결과가 나온다.





이스케이프(Escape) 문자

라인의 처음과 끝을 매칭하거나, 와일드 카드를 명세하기 위해서 정규 표현식의 특수 문자를 사용했기 때문에,
정규 표현식에 사용된 문자가 "정상(normal)"적인 문자임을 표기할 방법이 필요하고
달러 기호와 탈자 기호(^) 같은 실제 문자를 매칭하고자 한다.

역슬래쉬(')를 가진 문자를 앞에 덮붙여서 문자를 단순히 매칭하고자 한다고 나타낼 수 있다.
예를 들어, 다음 정규표현식으로 금액을 찾을 수 있다.




역슬래쉬 달러 기호를 앞에 덮붙여서, 실제로 "라인 끝(end of line)" 매칭 대신에 입력 문자열의 달러 기호와 매칭한다.
정규 표현식 나머지 부분은 하나 혹은 그 이상의 숫자 혹은 소수점 문자를 매칭한다.
주목: 꺾쇠 괄호 내부에 문자는 "특수 문자"가 아니다. 
그래서 "[0-9.]"은 실제 숫자 혹은 점을 의미한다.
꺾쇠 괄호 외부에 점은 "와일드 카드(wild-card)" 문자이고 임의의 문자와 매칭한다. 
꺾쇠 괄호 내부에서 점은 점일 뿐이다.


요약

지금까지 정규 표현식의 표면을 긁은 정도지만, 정규 표현식 언어에 대해서 조금 학습했다.
정규 표현식은 특수 문자로 구성된 검색 문자열로 "매칭(matching)" 정의하고 매칭된 문자열로부터 추출된 결과물을 정규 표현식 시스템과 프로그래머가 의도한 바를 의사소통하는 것이다.
다음에 특수 문자 및 문자 시퀀스의 일부가 있다.

^ 라인의 처음을 매칭.

 라인의 끝을 매칭.

. 임의의 문자를 매칭(와일드 카드)

s 공백 문자를 매칭.

S 공백이 아닌 문자를 매칭.(s 의 반대).

* 바로 앞선 문자에 적용되고 0 혹은 그 이상의 앞선 문자와 매칭을 표기함.

*? 바로 앞선 문자에 적용되고 0 혹은 그 이상의 앞선 문자와 매칭을 "탐욕적이지 않은(non-greedy) 방식"으로 표기함.

+ 바로 앞선 문자에 적용되고 1 혹은 그 이상의 앞선 문자와 매칭을 표기함.

+? 바로 앞선 문자에 적용되고 1 혹은 그 이상의 앞선 문자와 매칭을 "탐욕적이지 않은(non-greedy) 방식"으로 표기함.

[aeiou] 명세된 집합 문자에 존재하는 단일 문자와 매칭. 다른 문자는 안되고, "a", "e", "i", "o", "u" 문자만 매칭되는 예제.

[a-z0-9] 음수 기호로 문자 범위를 명세할 수 있다. 소문자이거나 숫자인 단일 문자만 매칭되는 예제.

[^A-Za-z] 집합 표기의 첫문자가 ^인 경우, 로직을 거꾸로 적용한다. 
대문자나 혹은 소문자가 아닌 임의 단일 문자만 매칭하는 예제.

( ) 괄호가 정규표현식에 추가될 때, 매칭을 무시한다. 
하지만 findall()을 사용할 때 전체 문자열보다 매칭된 문자열의 상세한 부속 문자열을 추출할 수 있게 한다.

b 빈 문자열을 매칭하지만, 단어의 시작과 끝에만 사용된다.

B 빈 문자열을 매칭하지만, 단어의 시작과 끝이 아닌 곳에 사용된다.

d 임의 숫자와 매칭하여 [0-9] 집합에 상응함.

D 임의 숫자가 아닌 문자와 매칭하여 [^0-9] 집합에 상응함.


유닉스 사용자를 위한 보너스

정규 표현식을 사용하여 파일을 검색 기능은 1960년대 이래로 유닉스 운영 시스템에 내장되어
여러가지 형태로 거의 모든 프로그래밍 언어에서 이용가능하다.

사실, search() 예제에서와 거의 동일한 기능을 하는 grep (Generalized Regular Expression Parser)으로 불리는 유닉스 내장 명령어 프로그램이 있다.
그래서, 맥킨토시나 리눅스 운영 시스템을 가지고 있다면, 명령어 창에서 다음 명령어를 시도할 수 있다.




grep을 사용하여, mbox-short.txt 파일 내부에 "From:" 문자열로 시작하는 라인을 보여준다.
grep 명령어를 가지고 약간 실험을 하고 grep에 대한 문서를 읽는다면, 파이썬에서 지원하는 정규 표현식과 grep에서 지원되는 정규 표현식과 차이를 발견할 것이다.
예를 들어, grep 공백이 아닌 문자 "S"을 지원하지 않는다. 그래서 약간 더 복잡한 집합 표기 "[^ ]"을 사용해야 한다.
"[^ ]"은 간단히 정리하면, 공복을 제외한 임의의 문자와 매칭한다.


디버깅

만약 특정 메쏘드의 정확한 이름을 기억해 내기 위해서 빠르게 생각나게 하는 것이 필요하다면 도움이 많이 될 수 있는 간단하고 초보적인 내장 문서가 파이썬에 포함되어 있다.
내장 문서 도움말은 인터랙티브 모드의 파이썬 인터프리터에서 볼 수 있다.

help()를 사용하여 인터랙티브 도움을 받을 수 있다.




특정한 모듈을 사용하고자 한다면, dir() 명령어를 사용하여 다음과 같이 모듈의 메쏘드를 찾을 수 있다.




또한, dir() 명령어를 사용하여 특정 메쏘드에 대한 짧은 문서 도움말을 얻을 수 있다.




내장 문서는 광범위하지 않아서, 급하거나, 웹 브라우저나 검색엔진에 접근할 수 없을 때 도움이 될 수 있다.


용어정의



[부서지기 쉬운 코드(brittle code):]
입력 데이터가 특정한 형식일 경우에만 작동하는 코드.
하지만 올바른 형식에서 약간이도 벗아나게 되면 깨지기 쉽니다.
쉽게 부서지기 때문에 "부서지기 쉬운 코드(brittle code)"라고 부른다.

[욕심쟁이 매칭(greedy matching):]
정규 표현식의 "+", "*" 문자는 가능한 큰 문자열을 매칭하기 위해서 밖으로 확장하는 개념.
[grep:]
정규 표현식에 매칭되는 파일을 탐색하여 라인을 찾는데 대부분의 유닉스 시스템에서 사용가능한 명령어.
"Generalized Regular Expression Parser"의 약자.
[정규 표현식(regular expression):]
좀더 복잡한 검색 문자열을 표현하는 언어. 
정규 표현식은 특수 문자를 포함해서 검색 라인의 처음 혹은 끝만 매칭하거나 많은 비슷한 것을 매칭한다.

[와일드 카드(wild card):]
임의 문자를 매칭하는 특수 문자. 정규 표현식에서 와이드 카드 문자는 마침표 문자다.

연습 문제


유닉스의 grep 명령어를 모사하는 간단한 프로그램을 작성하세요.
사용자가 정규 표현식을 입력하고 정규 표현식에 매칭되는 라인수를 셈하는 프로그램입니다.





다음 형식의 라인만을 찾는 프로그램을 작성하세요.

New Revision: 39772

그리고, 정규 표현식과 findall() 메쏘드를 사용하여 각 라인으로부터 숫자를 추출하세요.
숫자들의 평균을 구하고 출력하세요.









네트워크 프로그램
지금까지 책의 많은 예제는 파일을 읽고 파일의 정보를 찾는데 집중했지만, 
다양한 많은 정보의 원천이 인터넷에 있다.

이번 장에서는 웹브라우져로 가장하고 HTTP 프로토콜(HyperText Transport Protocol,HTTP)을 사용하여 웹페이지를 검색할 것이다. 
웹페이지 데이터를 읽고 파싱할 것이다.


하이퍼 텍스트 전송 프로토콜(HyperText Transport Protocol - HTTP)
웹에 동력을 공급하는 네트워크 프로토콜은 실제로 매우 단순하다.
파이썬에는 소켓 (sockets)이라고 불리는 내장 지원 모듈이 있다. 
파이썬 프로그램에서 소켓 모듈을 통해서 네트워크 연결을 하고, 데이터 검색을 매우 용이하게 한다.

소켓(socket)은 단일 소켓으로 두 프로그램 사이에 양방향 연결을 제공한다는 점을 제외하고 파일과 매우 유사하다.
동일한 소켓에 읽거나 쓸 수 있다. 
소켓에 무언가를 쓰게 되면, 소켓의 다른 끝에 있는 응용프로그램에 전송된다. 
소켓으로부터 읽게 되면, 다른 응용 프로그램이 전송한 데이터를 받게 된다.

하지만, 소켓의 다른쪽 끝에 프로그램이 어떠한 데이터도 전송하지 않았는데 소켓을 읽으려고 하면, 단지 앉아서 기다리기만 한다.
만약 어떠한 것도 보내지 않고 양쪽 소켓 끝의 프로그램 모두 기다리기만 한다면, 모두 매우 오랜 시간동안 기다리게 될 것이다.

인터넷으로 통신하는 프로그램의 중요한 부분은 특정 종류의 프로토콜을 공유하는 것이다.
프로토콜(protocol)은 정교한 규칙의 집합으로 누가 메시지를 먼저 보내고, 메세지로 무엇을 하며, 메시지에 대한 응답은 무엇이고, 다음에 누가 메세지를 보내고 등등을 포함한다.
이런 관점에서 소켓 끝의 두 응용프로그램이 함께 춤을 추고 있으니, 다른 사람 발을 밟지 않도록 확인해야 한다.

네트워크 프로토콜을 기술하는 문서가 많이 있다. 
하이퍼텍스트 전송 프로토콜(HyperText Transport Protocol)은 다음 문서에 기술되어 있다.

http://www.w3.org/Protocols/rfc2616/rfc2616.txt

매우 상세한 176 페이지나 되는 장문의 복잡한 문서다. 
흥미롭다면 시간을 가지고 읽어보기 바란다. 
RFC2616에 36 페이지를 읽어보면, GET 요청(request)에 대한 구문을 발견하게 된다. 
꼼꼼히 읽게 되면, 웹서버에 문서를 요청하기 하기 위해서, 80 포트로 www.py4inf.com 서버에 
연결을 하고 나서 다음 양식 한 라인을 전송한다.

GET http://www.py4inf.com/code/romeo.txt HTTP/1.0 

두번째 매개변수는 요청하는 웹페이지가 된다. 
그리고 또한 빈 라인도 전송한다. 
웹서버는 문서에 대한 헤더 정보와 빈 라인 그리고 문서 본문으로 응답한다.



세상에서 가장 간단한 웹 브라우져(Web Browser)
아마도 HTTP 프로토콜이 어떻게 작동하는지 알아보는 가장 간단한 방법은 매우 간단한 파이썬 프로그램을 작성하는 것이다.
웹서버에 접속하고 HTTP 프로토콜 규칙에 따라 문서를 요청하고 서버가 다시 보내주는 결과를 보여주는 것이다.




처음에 프로그램은 www.py4inf.com 서버에 80 포트로 연결한다.
"웹 브라우져" 역할로 작성된 프로그램이 하기 때문에 HTTP 프로토콜은 GET 명령어를 공백 라인과 함께 보낸다.


공백 라인을 보내자 마자, 512 문자 덩어리의 데이터를 소켓에서 받아 더 이상 읽을 데이터가 없을 때까지(즉, recv()이 빈 문자열을 반환한다.) 데이터를 출력하는 루프를 작성한다.

프로그램 실행결과 다음을 얻을 수 있다.




출력결과는 웹서버가 문서를 기술하기 위해서 보내는 헤더(header)로 시작한다.
예를 들어, Content-Type  헤더는 문서가 일반 텍스트 문서(text/plain)임을 표기한다.

서버가 헤더를 보낸 후에, 빈 라인을 추가해서 헤더 끝임을 표기하고 나서 실제 파일romeo.txt을 보낸다.

이 예제를 통해서 소켓을 통해서 저수준(low-level) 네트워크 연결을 어떻게 하는지 확인할 수 있다.
소켓을 사용해서 웹서버, 메일 서버 혹은 다른 종류의 서버와 통신할 수 있다.
필요한 것은 프로토콜을 기술하는 문서를 찾고 프로토콜에 따라 데이터를 주고 받는 코드를 작성하는 것이다.

하지만, 가장 흔히 사용하는 프로토콜은 HTTP (즉, 웹) 프로토콜이기 때문에, 
파이썬에는 HTTP 프로토컬을 지원하기 위해 특별히 설계된 라이브러리가 있다.
이것을 통해서 웹상에서 데이터나 문서를 검색을 쉽게 할 수 있다.


HTTP를 통해서 이미지 가져오기

상기 예제에서는 파일에 새줄(newline)이 있는 일반 텍스트 파일을 가져왔다. 
그리고 나서, 프로그램을 실행해서 데이터를 단순히 화면에 복사했다.
HTTP를 사용하여 이미지를 가져오도록 비슷하게 프로그램을 작성할 수 있다. 
프로그램 실행 시에 화면에 데이터를 복사하는 대신에,
데이터를 문자열로 누적하고, 다음과 같이 헤더를 잘라내고 나서 파일에 이미지 데이터를 저장한다. 




프로그램을 실행하면, 다음과 같은 출력을 생성한다.



상기 url에 대해서, Content-Type  헤더가 문서 본문이 이미지(image/jpeg)를 나타내는 것을 볼 수 있다.
프로그램이 완료되면, 이미지 뷰어로 stuff.jpg 파일을 열어서 이미지 데이터를 볼 수 있다.

프로그램을 실행하면, recv() 메쏘드를 호출할 때 마다 5120 문자는 전달받지 못하는 것을 볼 수 있다.
recv() 호출하는 순간마다 웹서버에서 네트워크로 전송되는 가능한 많은 문자를 받을 뿐이다. 
매번 5120 문자까지 요청하지만, 1460 혹은 2920 문자만 전송받는다. 

결과값은 네트워크 속도에 따라 달라질 수 있다. 
recv() 메쏘드 마지막 호출에는 스트림 마지막인 1681 바이트만 받았고,
recv() 다음 호출에는 0 길이 문자열을 전송받아서, 서버가 소켓 마지막에 close() 메쏘드를 호출하고 
더이상의 데이터가 없다는 신호를 준다.

주석 처리한 time.sleep()을 풀어줌으로써 recv() 연속 호출을 늦출 수 있다.
이런 방식으로 매번 호출 후에 0.25초 기다리게 한다.
그래서, 사용자가 recv() 메쏘드를 호출하기 전에 서버가 먼저 도착할 수 있어서 더 많은 데이터를 보낼 수가 있다.
정지 시간을 넣어서 프로그램을 다시 실행하면 다음과 같다.




recv() 메쏘드 호출의 처음과 마지막을 제외하고, 매번 새로운 데이터를 요청할 때마다 이제 5120 문자가 전송된다.

서버 send() 요청과 응용프로그램 recv() 요청 사이에 버퍼가 있다.
프로그램에 지연을 넣어 실행하게 될 때, 어느 지점엔가 서버가 소켓 버퍼를 채우고 응용프로그램이 버퍼를 비울 때까지 잠시 멈춰야 된다. 
송신하는 응용프로그램 혹은 수신하는 응용프로그램을 멈추게 하는 행위를 "흐름 제어(flow control)"이라고 한다.


urllib 사용하여 웹페이지 가져오기

수작업으로 소켓 라이브러리를 사용하여 HTTP로 데이터를 주고 받을 수 있지만,
urllib 라이브러리를 사용하여 파이썬에서 동일한 작업을 수행하는 좀더 간편한 방식이 있다.

urllib을 사용하여 파일처럼 웹페이지를 다룰 수가 있다.
단순하게 어느 웹페이지를 가져올 것인지만 지정하면 urllib 라이브러리가 모든 HTTP 프로토콜과 헤더관련 사항을 처리해 준다.

웹에서 romeo.txt 파일을 읽도록 urllib를 사용하여 작성한 상응 코드는 다음과 같다.




urllib.urlopen을 사용하여 웹페이지를 열게 되면, 파일처럼 다룰 수 있고 for 루프를 사용하여 데이터를 읽을 수 있다.

프로그램을 실행하면, 파일 내용 출력결과만을 볼 수 있다. 
헤더정보는 여전히 전송되었지만, urllib 코드가 헤더를 받아 내부적으로 처리하고, 사용자에게는 단지 데이터만 반환한다.




예제로, romeo.txt 데이터를 가져와서 파일의 각 단어 빈도를 계산하는 프로그램을 다음과 같이 작성할 수 있다.




다시 한번, 웹페이지를 열게 되면, 로컬 파일처럼 웹페이지를 읽을 수 있다.


HTML 파싱과 웹 스크래핑

파이썬 urllib 활용하는 일반적인 사례는 웹 스크래핑(scraping)이다.
웹 스크래핑은 웹브라우저를 가장한 프로그램을 작성하는 것이다.
웹페이지를 가져와서, 패턴을 찾아 페이지 내부의 데이터를 꼼꼼히 조사한다.
예로, 구글같은 검색엔진은 웹 페이지의 소스를 조사해서 다른 페이지로 가는 링크를 추출하고, 그 해당 페이지를 가져와서 링크 추출하는 작업을 반복한다.
이러한 기법으로 구글은 웹상의 거의 모든 페이지를 거미(spiders)줄처럼 연결한다.

구글은 또한 발견한 웹페이지에서 특정 페이지로 연결되는 링크 빈도를 사용하여 얼마나 중요한 페이지인지를 측정하고 
검색결과에 페이지가 얼마나 높은 순위로 노출되어야 하는지 평가한다.


정규 표현식 사용 HTML 파싱하기

HTML을 파싱하는 간단한 방식은 정규 표현식을 사용하여 특정한 패턴과 매칭되는 부속 문자열을 반복적으로 찾아 추출하는 것이다.

여기 간단한 웹페이지가 있다.




모양 좋은 정규표현식을 구성해서 다음과 같이 상기 웹페이지에서 링크를 매칭하고 추출할 수 있다.





작성된 정규 표현식은 "href="http://"로 시작하고, 하나 이상의 문자를 ".+?" 가지고 큰 따옴표를 가진 문자열을 찾는다.
".+?"에 물음표가 갖는 의미는 매칭이 "욕심쟁이(greedy)" 방식보다 "비욕심쟁이(non-greedy)" 방식으로 수행됨을 나타낸다. 
비욕심쟁이(non-greedy) 매칭방식은 가능한 가장 적게 매칭되는 문자열을 찾는 방식이고, 욕심 방식은 가능한 가장 크게 매칭되는 문자열을 찾는 방식이다.

추출하고자 하는 문자열이 매칭된 문자열의 어느 부분인지를 표기하기 위해서 정규 표현식에 괄호를 추가하여 다음과 같이 프로그램을 작성한다.




findall 정규 표현식 메쏘드는 정규 표현식과 매칭되는 모든 문자열 리스트를 추출하여 큰 따옴표 사이에 링크 텍스트만을 반환한다.

프로그램을 실행하면, 다음 출력을 얻게된다.




정규 표현식은 HTML이 예측가능하고 잘 구성된 경우에 멋지게 작동한다.
하지만, "망가진" HTML 페이지가 많아서, 정규 표현식만을 사용하는 솔류션은 유효한 링크를 놓치거나 잘못된 데이터만 찾고 끝날 수 있다.

이 문제는 강건한 HTML 파싱 라이브러리를 사용해서 해결될 수 있다.


BeautifulSoup 사용한 HTML 파싱
HTML을 파싱하여 페이지에서 데이터를 추출할 수 있는 파이썬 라이브러리는 많이 있다.
라이브러리 각각은 강점과 약점이 있어서 사용자 필요에 따라 취사선택한다.

예로, 간단하게 HTML 입력을 파싱하여 BeautifulSoup 라이브러리를 사용하여 링크를 추출할 것이다.
다음 웹사이트에서 BeautifulSoup 코드를 다운로드 받아 설치할 수 있다.

www.crummy.com

BeautifulSoup 라이브러리를 다운로드 받아 "설치"하거나 BeautifulSoup.py 파일을 응용프로그램과 동일한 폴더에 저장한다.

HTML이 XML 처럼 보이고 몇몇 페이지는 XML로 되도록 꼼꼼하게 구축되었지만, 
일반적으로 대부분의 HTML이 깨져서 XML 파서가 HTML 전체 페이지를 잘못 구성된 것으로 간주하고 받아들이지 않는다.
BeautifulSoup 라이브러리는 결점 많은 HTML 페이지에 내성이 있어서 사용자가 필요로하는 데이터를 쉽게 추출할 수 있게 한다.

urllib를 사용하여 페이지를 읽어들이고, BeautifulSoup를 사용해서 앵커 태그 (a)로부터 href 속성을  추출한다.




프로그램이 웹 주소를 입력받고, 웹페이지를 열고, 데이터를 읽어서 BeautifulSoup 파서에 전달하고,
그리고 나서 모든 앵커 태그를 불러와서 각 태그별로 href 속성을 출력한다.

프로그램을 실행하면, 아래와 같다.




BeautifulSoup을 사용하여 다음과 같이 각 태그별로 다양한 부분을 뽑아낼 수 있다.




상기 프로그램은 다음을 출력합니다.




HTML을 파싱하는데 BeautifulSoup이 가진 강력한 기능을 예제로 보여줬다.
좀더 자세한 사항은 www.crummy.com 에서 문서와 예제를 살펴보세요.


urllib을 사용하여 바이너리 파일 읽기

이미지나 비디오 같은 텍스트가 아닌 (혹은 바이너리) 파일을 가져올 때가 종종 있다.
일반적으로 이런 파일 데이터를 출력하는 것은 유용하지 않다. 
하지만, urllib을 사용하여, 하드 디스크 로컬 파일에 URL 사본을 쉽게 만들 수 있다.

이 패턴은 URL을 열고, read를 사용해서 문서 전체 내용을 다운로드하여 문자열 변수(img)에 다운로드하고,
그리고 나서 다음과 같이 정보를 로컬 파일에 쓴다.




작성된 프로그램은 네트워크로 모든 데이터를 한번에 읽어서 컴퓨터 주기억장치 img 변수에 저장하고,
cover.jpg 파일을 열어 디스크에 데이터를 쓴다. 
이 방식은 파일 크기가 사용자 컴퓨터의 메모리 크기보다 작다면 정상적으로 작동한다.

하지만, 오디오 혹은 비디오 파일 대용량이면, 상기 프로그램은 멈추거나 사용자 컴퓨터 메모리가 부족할 때 극단적으로 느려질 수 있다.
메모리 부족을 회피하기 위해서, 데이터를 블럭 혹은 버퍼로 가져와서, 다음 블럭을 가져오기 전에 디스크에 각각의 블럭을 쓴다.
이런 방식으로 사용자가 가진 모든 메모리를 사용하지 않고 어떠한 크기 파일도 읽어올 수 있다.




상기 예제에서, 한번에 100,000 문자만 읽어 오고, 웹에서 다음 100,000 문자를 가져오기 전에 cover.jpg 파일에 읽어온 문자를 쓴다.

프로그램 실행 결과는 다음과 같다.




UNIX 혹은 매킨토시 컴퓨터를 가지고 있다면, 다음과 같이 상기 동작을 수행하는 명령어가 운영체제 자체에 내장되어 있다.




curl은 "copy URL"의 단축 명령어로 두 예제는 curl 명령어와 비슷한 기능을 구현해서, 
www.py4inf.com/code 사이트에 curl1.py, curl2.py 이름으로 올라가 있다.
동일한 작업을 좀더 효과적으로 수행하는 curl3.py 샘플 프로그램도 있어서 실무적으로 작성하는 프로그램에 이런한 패턴을 이용하여 구현할 수 잇다.


용어정의



[BeautifulSoup:] 파이썬 라이브러리로 HTML 문서를 파싱하고 
브라우저가 일반적으로 생략하는 HTML의 불완전한 부분을 보정하여 HTML 문서에서 데이터를 추출한다.
www.crummy.com 사이트에서 BeautifulSoup 코드를 다운로드 받을 수 있다.
[포트(port):] 서버에 소켓 연결을 만들 때, 사용자가 무슨 응용프로그램을 연결하는지 나타내는 숫자.
예로, 웹 트래픽은 통상 80 포트, 전자우편은 25 포트를 사용한다.
[스크래핑(scraping):] 프로그램이 웹브라우저를 가장하여 웹페이지를 가져와서 웹 페이지의 내용을 검색한다.
종종 프로그램이 한 페이지의 링크를 따라 다른 페이지를 찾게 된다. 그래서, 웹페이지 네트워크 혹은 소셜 네트워크 전체를 훑을 수 있다.
[소켓(socket):] 두 응용프로그램 사이 네트워크 연결. 두 응용프로그램은 양 방향으로 데이터를 주고 받는다.
[스파이더(spider):] 검색 색인을 구축하기 위해서 한 웹페이지를 검색하고, 그 웹페이지에 링크된 모든 페이지 검색을 반복하여
인터넷에 있는 거의 모든 웹페이지를 가져오기 위해서 사용되는 검색엔진 행동.

연습문제


소켓 프로그램 socket1.py을 변경하여 임의 웹페이지를 읽을 수 있도록 URL을 사용자가 입력하도록 바꾸세요.
split('/')을 사용하여 URL을 컴포턴트로 쪼개서 소켓 connect 호출에 대해 호스트 명을 추출할 수 있다.
사용자가 적절하지 못한 형식 혹은 존재하지 않는 URL을 입력하는 경우를 처리할 있도록 try, except를 사용하여 오류 검사기능을 추가하세요.

소켓 프로그램을 변경하여 전송받은 문자를 계수(count)하고 3000 문자를 출력한 후에 그이상 텍스트 출력을 멈추게 하세요.
프로그램은 전체 문서를 가져와야 하고, 전체 문자를 계수(count)하고, 문서 마지막에 문자 계수(count)결과를 출력해야 합니다.

urllib을 사용하여 이전 예제를 반복하세요. (1) 사용자가 입력한 URL에서 문서 가져오기
(2) 3000 문자까지 화면에 보여주기 (3) 문서의 전체 문자 계수(count)하기.
이 연습문제에서 헤더에 대해서는 걱정하지 말고, 단지 문서 본문에서 첫 3000 문자만 화면에 출력하세요.
 urllinks.py 프로그램을 변경하여 가져온 HTML 문서에서 문단 (p) 태그를 추출하고
프로그램의 출력물로 문단을 계수(count)하고 화면에 출력하세요. 
문단 텍스트를 화면에 출력하지 말고 단지 숫자만 셉니다.
작성한 프로그램을 작은 웹페이지 뿐만 아니라 조금 큰 웹 페이지에도 테스트해 보세요.

(고급) 소켓 프로그램을 변경하여 헤더와 빈 라인 다음에 데이터만 보여지게 하세요.
recv는 라인이 아니라 문자(새줄(newline)과 모든 문자)를 전송받는다는 것을 기억하세요.





웹서비스 사용하기
프로그램을 사용하여 HTTP상에서 문서를 가져와서 파싱하는 것이 익숙해지면, 
다른 프로그램(즉, 브라우져에서 HTML로 보여지지 않는 것)에서 활용되도록 특별히 설계된 문서를 생성하는 것은 그다지 오래 걸리지 않는다.

웹상에서 데이터를 교환할 때 두 가지 형식이 많이 사용된다.
XML("eXtensible Markup Language")은 오랜 기간 사용되어져 왔고 문서-형식(document-style) 데이터를 교환하는데 가장 적합하다.
딕셔너리, 리스트 혹은 다른 내부 정보를 프로그램으로 서로 교환할 때, JSON(JavaScript Object Notation, www.json.org)을 사용한다. 
두 가지 형식에 대해 모두 살펴볼 것이다.


XML(eXtensible Markup Language)
XML은 HTML과 매우 유사하지만, XML이 좀더 HTML보다 구조화되었다.
여기 XML 문서 샘플이 있다.




종종 XML문서를 나무 구조(tree structure)로 생각하는 것이 도움이 된다. 
최상단 person 태그가 있고, phone 같은 다른 태그는 부모 노드의 자식(children) 노드로 표현된다.



XML 파싱

다음은 XML을 파싱하고 XML에서 데이터 요소를 추출하는 간단한 응용프로그램이다.




fromstring을 호출하여 XML 문자열 표현을 XML 노드 '나무(tree)'로 변환한다.
XML이 나무구조로 되었을 때, XML에서 데이터 일부분을 추출하기 위해서 호출하는 메쏘드가 연달아 있다.

find 함수는 XML 나무를 훑어서 특정한 태그와 매칭되는 노드(node)를 검색한다. 
각 노드는 텍스트, 속성(즉, hide 같은), 그리고 "자식(child)" 노드로 구성된다. 
각 노드는 노드 나무의 최상단이 될 수 있다.




ElementTree같은 XML 파서를 사용하는 것은 장점이 있다.
상기 예제의 XML은 매우 간단하지만,
적합한 XML에 관해서 규칙이 많이 있고, XML 구문 규칙에 얽매이지 않고 ElementTree를 사용해서 XML에서 데이터를 추출할 수 있다.


노드 반복하기

종종 XML이 다중 노드를 가지고 있어서 모든 노드를 처리하는 루프를 작성할 필요가 있다.
다음 프로그램에서 모든 user 노드를 루프로 반복한다.




findall 메쏘드는 파이썬 리스트의 하위 나무를 가져온다.
리스트는 XML 나무에서 user 구조를 표현한다. 
그리고 나서, for 루프를 작성해서 각 user 노드 값을 확인하고 name, id 텍스트 요소와 user 노드에서 x 속성도 출력한다.





JSON(JavaScript Object Notation)
JSON 형식은 자바스크립트 언어에서 사용되는 객체와 배열 형식에서 영감을 얻었다.
하지만 파이썬이 자바스크립트 이전에 개발되어서 딕셔너리와 리스트의 파이썬 구문이 JSON 구문에 영향을 주었다.
그래서 JSON 포맷이 거의 파이썬 리스트와 딕셔너리의 조합과 일치한다. 

상기 간단한 XML에 대략 상응하는 JSON으로 작성한 것이 다음에 있다.




몇가지 차이점에 주목하세요. 첫째로 XML에서는 "phone" 태그에 "intl"같은 속성을 추가할 수 있다.
JSON 에서는 단지 키-값 페어(key-value pair)다. 
또한 XML "person" 태그는 사라지고 외부 중괄호 세트로 대체되었다.

일반적으로 JSON 구조가 XML 보다 간단하다. 
왜냐하면, JSON 이 XML보다 적은 역량을 보유하기 때문이다.
하지만 JSON 이 딕셔너리와 리스트의 조합에 직접 매핑된다는 장점이 있다.
그리고, 거의 모든 프로그래밍 언어가 파이썬 딕셔너리와 리스트에 상응하는 것을 갖고 있어서,
JSON 이 협업하는 두 프로그램 사이에서 데이터를 교환하는 매우 자연스러운 형식이 된다.

XML 에 비해서 상대적으로 단순하기 때문에, JSON 이 응용프로그램 간 거의 모든 데이터를 교환하는데 있어 빠르게 선택되고 있다. 


JSON 파싱하기
딕셔너리(객체)와 리스트를 중첩함으로써 JSON을 생성한다. 
이번 예제에서, user 리스트를 표현하는데, 각 user가 키-값 페어(key-value pair, 즉, 딕셔너리)다.
그래서 리스트 딕셔너리가 있다.

다음 프로그램에서 내장된 json 라이브러리를 사용하여 JSON을 파싱하여 데이터를 읽어온다.
이것을 상응하는 XML 데이터, 코드와 비교해 보세요. 
JSON은 조금 덜 정교해서 사전에 미리 리스트를 가져오고, 리스트가 사용자이고, 각 사용자가 키-값 페어 집합임을 알고 있어야 한다. 
JSON은 좀더 간략(장점)하고 하지만 좀더 덜 서술적(단점)이다. 




JSON과 XML에서 데이터를 추출하는 코드를 비교하면, json.loads()을 통해서 파이썬 리스트를 얻는다.
for 루프로 파이썬 리스트를 훑고, 리스트 내부의 각 항목은 파이썬 딕셔너리로 각 사용자별 다양한 정보를 추출하기 위해서 파이썬 인덱스 연산자를 사용한다. 
JSON을 파싱하면, 네이티브 파이썬 객체와 구조가 생성된다.
반환된 데이터가 단순히 네이티브 파이썬 구조체이기 때문에, 파싱된 JSON을 활용하는데 JSON 라이브러리를 사용할 필요는 없다. 
 
프로그램 출력은 정확하게 상기 XML 버젼과 동일한다.




일반적으로 웹서비스에 대해서 XML에서 JSON으로 옮겨가는 산업 경향이 뚜렷하다.
JSON이 프로그래밍 언어에서 이미 갖고 있는 네이티브 자료 구조와 좀더 직접적이며 간단히 매핑되기 때문에,
JSON을 사용할 때 파싱하고 데이터 추출하는 코드가 더욱 간단하고 직접적이다.
하지만 XML 이 JSON 보다 좀더 자기 서술적이고 XML 이 강점을 가지는 몇몇 응용프로그램 분야가 있다.
예를 들어, 대부분의 워드 프로세서는 JSON보다는 XML을 사용하여 내부적으로 문서를 저장한다. 


API(Application Program Interfaces, 응용 프로그램 인터페이스)

이제 HTTP를 사용하여 응용프로그램간에 데이터를 교환할 수 있게 되었다. 
또한, XML 혹은 JSON을 사용하여 응용프로그램간에도 복잡한 데이터를 주고 받을 수 있는 방법을 습득했다.

다음 단계는 상기 학습한 기법을 사용하여 응용프로그램 간에 "계약(contract)"을 정의하고 문서화한다.
응용프로그램-대-응용프로그램 계약에 대한 일반적 명칭은 API 응용 프로그램 인터페이스(Application Program Interface) 다. 
API를 사용할 때, 일반적으로 하나의 프로그램이 다른 응용 프로그램에서 사용할 수 있는 가능한 서비스 집합을 생성한다.
또한, 다른 프로그램이 서비스에 접근하여 사용할 때 지켜야하는 API (즉, "규칙")도 게시한다.  

다른 프로그램에서 제공되는 서비스에 접근을 포함하여 프로그램 기능을 개발할 때, 
이러한 개발법을 SOA, Service-Oriented Architecture(서비스 지향 아키텍처)라고 부른다.
SOA 개발 방식은 전반적인 응용 프로그램이 다른 응용 프로그램 서비스를 사용하는 것이다. 
반대로, SOA가 아닌 개발방식은 응용 프로그램이 하나의 독립된 응용 프로그램으로 구현에 필요한 모든 코드를 담고 있다.

웹을 사용할 때 SOA 사례를 많이 찾아 볼 수 있다. 
웹사이트 하나를 방문해서 비행기표, 호텔, 자동차를 단일 사이트에서 예약완료한다. 
호텔관련 데이터는 물론 항공사 컴퓨터에 저장되어 있지 않다.
대신에 항공사 컴퓨터는 호텔 컴퓨터와 계약을 맺어 호텔 데이터를 가져와서 사용자에게 보여준다.
항공사 사이트를 통해서 사용자가 호텔 예약을 동의할 경우, 항공사 사이트에서 호텔 시스템의 또다른 웹서비스를 통해서 실제 예약을 한다.
전체 거래(transaction)를 완료하고 카드 결재를 진행할 때, 다른 컴퓨터가 프로세스에 관여하여 처리한다.


서비스 지향 아키텍쳐는 많은 장점이 있다. 
(1) 항상 단 하나의 데이터만 유지관리한다. 
이중으로 중복 예약을 원치 않는 호텔  같은 경우에 매우 중요하다. 
(2) 데이터 소유자가 데이터 사용에 대한 규칙을 정한다. 
이러한 장점으로, SOA 시스템은 좋은 성능과 사용자 요구를 모두 만족하기 위해서 신중하게 설계되어야 한다. 

응응프로그램이 웹상에 이용가능한 API로 서비스 집합을 만들 때, 웹서비스(web services)라고 부른다.


구글 지오코딩 웹서비스(Google Geocoding Web Service)
구글이 자체적으로 구축한 대용량 지리 정보 데이터베이스를 누구나 이용할 수 있게 하는 훌륭한 웹서비스가 있다.
"Ann Arbor, MI" 같은 지리 검색 문자열을 지오코딩 API 에 넣으면, 
검색 문자열이 의미하는 지도상에 위치와 근처 주요 지형지물 정보를 나름 최선을 다해서 예측 제공한다.  

지오코딩 서비스는 무료지만 사용량이 제한되어 있어서, 상업적 응용프로그램에 API를 무제한 사용할 수는 없다.
하지만, 최종 사용자가 자유형식 입력 박스에 위치정보를 입력하는 설문 데이터가 있다면,
구글 API를 사용하여 데이터를 깔끔하게 정리하는데는 유용하다.

구글 지오코딩 API 같은 무료 API를 사용할 때, 자원 사용에 대한 지침을 준수해야 한다.
너무나 많은 사람이 서비스를 남용하게 되면, 구글은 무료 서비스를 중단하거나, 상당부분 줄일 수 있다. 
(When you are using a free API like Google's geocoding API, you need
to be respectful in your use of these resources.  If too many people abuse the
service, Google might drop or significantly curtail its free service.
서비스에 대해서 자세한 사항을 온라인 문서를 정독할 수 있지만, 
무척 간단해서 브라우저에 다음 URL을 입력해서 테스트까지 할 수 있다.

http://maps.googleapis.com/maps/api/geocode/json?sensor=false &address=Ann+Arbor

웹프라우저에 붙여넣기 전에, URL만 뽑아냈고 URL에서 모든 공백을 제거했는지 확인하세요.

다음은 간단한 응용 프로그램이다. 
사용자가 검색 문자열을 입력하고 구글 지오코딩 API를 호출하여 반환된 JSON에서 정보를 추출한다.




프로그램이 사용자로부터 검색 문자열을 받는다. 
적절히 인코딩된 매개 변수로 검색문자열을 변환하여 URL을 만든다.
그리고 나서 urllib을 사용하여 구글 지오코딩 API에서 텍스트를 가져온다.
고정된 웹페이지와 달리, 반환되는 데이터는 전송한 매개변수와 구글 서버에 저장된 지리정보 데이터에 따라 달라진다.

JSON 데이터를 가져오면, json 라이브러리로 파싱하고 전송받은 데이터가 올바른지 확인하는 몇가지 절차를 거친 후에 찾고자 하는 정보를 추출한다.

프로그램 출력결과는 다음과 같다. (몇몇 JSON 출력은 의도적으로 삭제했다.)




다양한 구글 지오코딩 API의 XML과 JSON을 좀더 살펴보기 위해서 www.py4inf.com/code/geojson.py, www.py4inf.com/code/geoxml.py을 다운로드 받아보기 바란다.


보안과 API 사용
상용업체 API를 사용하기 위해서는 일종의 "API키(API key)"가 일반적으로 필요하다.
서비스 제공자 입장에서 누가 서비스를 사용하고 있으며 각 사용자가 얼마나 사용하고 있는를 알고자 한다.
상용 API 제공업체는 서비스에 대한 무료 사용자와 유료 사용자에 대한 구분을 두고 있다.
특정 기간 동안 한 개인 사용자가 사용할 수 있는 요청수에 대해 제한을 두는 정책을 두고 있다. 

때때로 API키를 얻게 되면, API를 호출할 때 POST 데이터의 일부로 포함하거나 URL의 매개변수로 키를 포함시킨다.

또 다른 경우에는 업체가 서비스 요청에 대한 보증을 강화해서 공유키와 비밀번호를 암호화된 메시지 형식으로 보내도록 요구한다. 
인터넷을 통해서 서비스 요청을 암호화하는 일반적인 기술을 OAuth라고 한다.
http://www.oauth.net 사이트에서 OAuth 프로토콜에 대해 더 많은 정보를 만날 수 있다. 

트위터 API가 점차적으로 가치있게 됨에 따라 트위터가 공개된 API에서 API를 매번 호출할 때마다 OAuth 인증을 거치도록 API를 바뀌었다. 
다행스럽게도 편리한 OAuth 라이브러리가 많이 있다. 

그래서 명세서를 읽고 아무것도 없는 상태에서 OAuth 구현하는 것을 필할 수 있게 되었다. 
이용 가능한 라이브러리는 복잡성도 다양한만틈 기능적으로도 다양하다. 
 OAuth 웹사이트에서 다양한 OAuth 라이브러리 정보를 확인할 수 있다. 

다음 샘플 프로그램으로 www.py4inf.com/code 사이트에서 twurl.py, hidden.py, 
oauth.py, twitter1.py 파일을 다운로드 받아서 컴퓨터 한 폴더에 저장한다.

프로그램을 사용하기 위해서 트위터 계정이 필요하고, 파이썬 코드를 응용프로그램으로 인증하고,
키, 암호, 토큰, 토큰 암호를 설정해야 한다. 
hidden.py 파일을 편집하여 4개 문자열을 파일에 적절한 변수에 저장한다.




트위터 웹서비스는 다음 URL을 사용하여 접근한다.

https://api.twitter.com/1.1/statuses/user_timeline.json

하지만, 모든 비밀 정보가 추가되면, URL은 다음과 같이 보인다.




OAuth 보안 요구사항을 충족하기 위해 추가된 다양한 매개 변수 의미를 좀더 자세히 알고자 한다면,
OAuth 명세서를 읽어보기 바란다.

트위터로 실행한 프로그램에서 oauth.py, twurl.py 두개 파일에 모든 복잡함을 감추었다.
hidden.py에 암호를 설정해서 URL을 twurl.augment() 함수에 전송했고,
라이브러리 코드는 URL에 필요한 매개 변수를 추가했다.

twitter1.py 프로그램은 특정 트위터 사용자 타임라인을 가져와서 JSON 형식 문자열로 반환한다.
단순하게 문자열의 첫 250 문자만 출력한다.




프로그램을 실행하면 다음 결과물을 출력한다.




반환된 타임라인 데이터와 함께 트위터는 또한 HTTP 응답 헤더에 요청사항에 대한 메타 데이터도 반환한다.
특히 헤더에 있는  x-rate-limit-remaining정보는 한동안 서비스를 이용 못하게 되기 전까지 얼마나 많은 요청을 할 수 있는가하는 정보를 담고 있다. 
API 요청을 매번 할 때마다 남은 숫자가 줄어드는 것을 확인할 수 있다.

다음 예제애서, 사용자의 트위터 친구 정보를 가져와서 JSON 파싱을 하고 친구에 대한 정보를 추출한다.
파싱 후에 JSON을 가져 와서, 좀더 많은 필드를 추출할 때 데이터를 자세히 살펴보는데 도움이 되도록 문자 4개로 들여쓰기한 "보기좋은 출력(pretty-print)"을 한다.



JSON은 중첩된 파이썬 리스트와 딕셔너리 집합이기 때문에, 
인덱스 연산과 for 루프를 조합해서 매우 적은 양의 코드로 반환된 데이터 구조를 훑어볼 수 있다.

프로그램 결과는 다음과 같다. (페이지에 맞도록 몇몇 데이터 항목을 줄였다.)




출력 마지막은 drchuck 트위터 계정에서 가장 최근 친구 5명을 for 루프로 읽고 친구의 가장 마지막 상태 정보를 출력한다. 
반환된 JSON에는 이용가능한 더 많은 데이터가 있다.
또한, 프로그램 출력을 보게 되면, 특정 계정의 "find the friends"가 일정 기간동안 실행 가능한 타임라인 질의 숫자와 다른 사용량에 제한을 두고 있음을 볼 수 있다.

이와 같은 보안 API키는 누가 트위터 API를 사용하고 어느 정도 수준으로 트위터를 사용하는지에 대해서 트위트가 확고한 신뢰를 갖게 한다. 
사용량에 한계를 두고 서비스를 제공하는 방식은 단순히 개인적인 목적으로 데이터 검색을 할 수는 있지만, 하루에 수백만 API 호출로 데이터를 추출하여 제품을 개발 못하게 제한하는 기능도 동시에 한다.


용어정의



[API:] 응용 프로그램 인터페이스(Application Program Interface) - 
두 응용 프로그램 컴포넌트 간에 상호작용하는 패턴을 정의하는 응용 프로그램 간의 계약.
[ElementTree:] XML데이터를 파싱하는데 사용되는 파이썬 내장 라이브러리.
[JSON:] JavaScript Object Notation- 자바스크립트 객체(JavaScript Objects) 구문을 기반으로
구조화된 데이터 마크업(markup)을 허용하는 형식.
[REST:] REpresentational State Transfer -
HTTP 프로토콜을 사용하여 응용 프로그램 내부에 자원에 접근을 제공하는 일종의 웹서비스 스타일.
[SOA:] 서비스 지향 아키텍처(Service Oriented Architecture) - 
응용 프로그램이 네트워크에 연결된 컴포넌트로 구성될 때.
[XML:] 확장 마크업 언어(eXtensible Markup Language) - 
구조화된 데이터의 마크업을 허용하는 형식.

Exercises


데이터를 가져와서 두 문자 국가 코드를 출력하도록 www.py4inf.com/code/geojson.py 혹은
www.py4inf.com/code/geoxml.py을 수정하세요.
오류 검사 기능을 추가하여 국가 코드가 없더라도 프로그램이 역추적(traceback)이 생성하지 않도록 하세요.
프로그램이 정상 작동하면, "Atlantic Ocean"을 검색하고 어느 국가에도 속하지 않는 지역을 처리할 수 있는지 확인하세요. 





데이터베이스와 SQL(Structured Query Language) 사용하기


데이터베이스가 뭔가요?
데이터베이스(database)는 데이터를 저장하기 위한 목적으로 조직된 파일이다. 
대부분의 데이터베이스는 키(key)와 값(value)를 매핑한다는 의미에서 딕셔너리처럼 조직되었다.
가장 큰 차이점은 데이터베이스는 디스크(혹은 다른 영구 저장소)에 위치하고 있어서, 프로그램 종료 후에도 정보가 계속 저장된다.
데이터베이스가 영구 저장소에 저장되어서, 컴퓨터 주기억장치(memory) 크기에 제한받는 딕셔너리보다 훨씬 더 많은 정보를 저장할 수 있다.

딕셔너리처럼, 데이터베이스 소프트웨어는 엄청난 양의 데이터 조차도 매우 빠르게 삽입하고 접근하도록 설계되었다.
컴퓨터가 특정 항목으로 빠르게 찾아갈 수 있도록 데이터베이스에 인덱스(indexes)를 추가한다.
데이터베이스 소프트웨어는 인덱스를 구축하여 성능을 보장한다.

다양한 목적에 맞춰 서로 다른 많은 데이터베이스 시스템이 개발되어 사용되고 있다. 
Oracle, MySQL, Microsoft SQL Server, PostgreSQL, SQLite이 여기에 포함된다. 
이 책에서는 SQLite를 집중해서 살펴볼 것이다. 
왜냐하면 매우 일반적인 데이터베이스이며 파이썬에 이미 내장되어 있기 때문이다.
응용프로그램 내부에서 데이터베이스 기능을 제공하도록 SQLite가 다른 응용프로그램 내부에 내장(embedded)되도록 설계되었다.
예를 들어, 다른 많은 소프트웨어 제품이 그렇듯이, 파이어폭스 브라우져도 SQLite를 사용한다.

http://sqlite.org/

이번 장에서 기술하는 트위터 스파이더링 응용프로그램처럼 정보과학(Informatics)에서 마주치는 몇몇 데이터 조작 문제에 SQLite가 적합하다.



데이터베이스 개념
처음 데이터베이스를 볼때 드는 생각은 마치 엑셀같은 다중 시트를 지닌 스프레드쉬트(spreadsheet)같다는 것이다.
데이터베이스에서 주요 데이터 구조물은 테이블(tables), 행(rows), and 열(columns)이 된다. 


관계형 데이터베이스의 기술적인 면을 설명하면 테이블, 행, 열의 개념은 
관계(relation), 튜플(tuple), and 속성(attribute) 각각 형식적으로 매칭된다.
이번 장에서는 조금 덜 형식 용어를 사용한다.


파이어폭스 애드온 SQLite 매니저
SQLite 데이터베이스 파일에 있는 데이터를 다루기 위해서 이번장에서 주로 파이썬 사용에 집중을 하지만, 
다음 웹사이트에서 무료로 이용 가능한 SQLite 데이터베이스 매니저(SQLite Database Manager)로 불리는 
파이어폭스 애드온(add-on)을 사용해서 좀더 쉽게 많은 작업을 수행할 수 있다.

https://addons.mozilla.org/en-us/firefox/addon/sqlite-manager/

브라우져를 사용해서 쉽게 테이블을 생성하고, 데이터를 삽입, 편집하고 데이터베이스 데이터에 대해 간단한 SQL 질의를 실행할 수 있다.

이러한 점에서 데이터베이스 매니저는 텍스트 파일을 작업할 때 사용하는 텍스트 편집기와 유사하다.
텍스트 파일에 하나 혹은 몇개 작업만 수행하고자 하면, 텍스트 편집기에서 파일을 열어 필요한 수정작업을 하고 닫으면 된다.
텍스트 파일에 작업할 사항이 많은 경우는 종종 간단한 파이썬 프로그램을 작성하여 수행한다.
데이터베이스로 작업할 때도 동일한 패턴이 발견된다. 
간단한 작업은 데이터베이스 매니저를 통해서 수행하고,
좀더 복잡한 작업은 파이썬으로 수행하는 것이 더 편리하다.



데이터베이스 테이블 생성하기

데이터베이스는 파이썬 리스트 혹은 딕셔너리보다 좀더 명확히 정의된 구조를 요구한다.(
실질적으로 SQLite는 열에 저장되는 데이터 형식에 대해서 좀더 많은 유연성을 부여하지만,
이번 장에서는 데이터 형식을 엄격하게 적용해서 MySQL 같은 다른 관계형 데이터베이스 시스템에도 동일한 개념이 적용되게 한다.).  

데이터베이스에 테이블(table)을 생성할 때, 열(column)의 명칭과 각 열(column)에 저장하는 테이터 형식을 사전에 정의해야 한다. 
데이터베이스 소프트웨어가 각 열의 데이터 형식을 인식하게 되면, 데이터 형식에 따라 데이터를 저장하고 찾아오는 방법을 가장 효율적인 방식을 선택할 수 있다.

다음 url에서 SQLite에서 지원되는 다양한 데이터 형식을 살펴볼 수 있다.

http://www.sqlite.org/datatypes.html

처음에는 데이터 구조를 사전에 정의하는 것이 불편하게 보이지만, 대용량의 데이터가 데이터베이스에 포함되더라도 데이터의 빠른 접근을 보장하는 잇점이 있다.

데이터베이스 파일과 데이터베이스에 두개의 열을 가진 Tracks 이름의 테이블을 생성하는 코드는 다음과 같다.




연결 (connect) 연산은 현재 디렉토리 music.sqlite3 파일에 저장된 데이터베이스에 "연결(connection)"한다.
파일이 존재하지 않으면, 자동 생성된다. 
"연결(connection)"이라고 부르는 이유는 때때로 데이터베이스가 응용프로그램이 실행되는 서버로부터
분리된 "데이터베이스 서버(database server)"에 저장되기 때문이다.
지금 간단한 예제 파일의 경우에 데이터베이스가 로컬 파일 형태로 파이썬 코드 마찬가지로 동일한 디렉토리에 있다.

파일을 다루는 파일 핸들(file handle)처럼 데이터베이스에 저장된 파일에 연산을 수행하기 위해서 커서(cursor)를 사용한다.
cursor()를 호출하는 것은 개념적으로 텍스트 파일을 다룰 때 open()을 호출하는 것과 개념적으로 매우 유사하다.


커서가 생성되면, execute() 메쏘드를 사용하여 데이터베이스 콘텐츠에 명령어 실행을 할 수 있다.

데이터베이스 명령어는 특별한 언어로 표현된다.
단일 데이터베이스 언어를 학습하도록 서로 다른 많은 데이터베이스 업체 사이에서 표준화되었다.

데이터베이스 언어를 SQL(Structured Query Language 구조적 질의 언어)로 부른다.

http://en.wikipedia.org/wiki/SQL

상기 예제에서, 데이터베이스에 두개의 SQL 명령어를 실행했다. 
관습적으로 데이터베이스 키워드는 대문자로 표기한다.
테이블명이나 열의 명칭처럼 사용자가 추가한 명령어 부분은 소문자로 표기한다.

첫 SQL 명령어는 만약 존재한다면 데이터베이스에서 Tracks 테이블을 삭제한다.
동일한 프로그램을 실행해서 오류 없이 반복적으로 Tracks 테이블을 생성하도록하는 패턴이다.
DROP TABLE 명령어는 데이터베이스 테이블 및 테이블 콘텐츠 전부를 삭제하니 주의한다. (즉, "실행취소(undo)"가 없다.)




두번째 명령어는 title 문자형 열과 plays 정수형 열을 가진 Tracks으로 명명된 테이블을 생성한다.




이제 Tracks으로 명명된 테이블을 생성했으니, SQL INSERT 연산을 통해 테이블에 데이터를 넣을 수 있다.
다시 한번, 데이터베이스에 연결하여 커서(cursor)를 얻어 작업을 시작한다. 
그리고 나서 커서를 사용해서 SQL 명령어를 수행한다.

SQL INSERT 명령어는 어느 테이블을 사용할지 특정한다.
그리고 나서 (title, plays)  포함할 필드 목록과 테이블 새로운 행에 저장될 VALUES 나열해서 신규 행을 정의를 마친다.
실제 값이 execute() 호출의 두번째 매개변수로 튜플( 'My Way', 15 ) 로 넘겨는 것을 표기하기 위해서 값을 물음표 (?, ?)로 명기한다.




먼저 테이블에 두개 열을 삽입(INSERT)하고 commit() 명령어를 사용하여 데이터가 데이터베이스에 저장되도록 했다.


그리고 나서, SELECT 명령어를 사용하여 테이블에 방금 전에 삽입한 행을 불러왔다.
SELECT 명령어에서 데이터를 어느 열(title, plays)에서, 어느 테이블Tracks에서 을 가져올지 명세한다.
SELECT 명령문을 수행한 후에, 커서는 for문 반복을 수행하는 것과 같다.
효율성을 위해서, 커서가 SELECT 명령문을 수행할 때 데이터베이스에서 모든 데이터를 읽지 않는다. 
대신에 for문에서 행을 반복해서 가져하듯이 요청시에만 데이터를 읽어온다.

프로그램 실행결과는 다음과 같다.



for 루프가 행을 두개를 읽어왔다. 
각각의 행은 title로 첫번째 값을,plays로 두번째 값을 갖는 파이썬 튜플이다. 
title 문자열이 u'로 시작한다고 걱정하지 마라.
해당 문자열은 라틴 문자가 아닌 다국어를 저장할 수 있는 유니코드(Unicode) 문자열을 나타내는 것이다.

프로그램 마지막에 SQL 명령어를 실행 사용해서 방금전에 생성한 행을 모두 삭제(DELETE)했기 때문에 
프로그램을 반복해서 실행할 수 있다. 
삭제(DELETE) 명령어는 WHERE 문을 사용하여 선택 조건을 표현할 수 있다.
따라서 명령문에 조건을 충족하는 행에만 명령문을 적용한다.
이번 예제에서 기준이 모든 행에 적용되어 테이블에 아무 것도 없게 된다.
따라서 프로그램을 반복적으로 실행할 수 있다.
삭제(DELETE)를 실행한 후에 commit()을 호출하여 데이터베이스에서 데이터를 완전히 제거했다.


SQL(Structured Query Language) 요약

지금까지, 파이썬 예제를 통해서 SQL(Structured Query Language)을 사용했고, SQL 명령어에 대한 기본을 다루었다.
이번 장에서는 SQL 언어를 보고 SQL 구문 개요를 살펴본다.

대단히 많은 데이터베이스 업체가 존재하기 때문에 호환성의 문제로 SQL(Structured Query Language)이 표준화되었다.
그래서, 여러 업체가 개발한  데이터베이스 시스템 사이에 호환하는 방식으로 커뮤니케이션 가능하다.

관계형 데이터베이스는 테이블, 행과 열로 구성된다. 
열(column)은 일반적으로 텍스트, 숫자, 혹은 날짜 자료형을 갖는다.
테이블을 생성할 때, 열의 명칭과 자료형을 지정한다.




테이블에 행을 삽입하기 위해서 SQL INSERT 명령어를 사용한다.




INSERT 문장은 테이블 이름을 명기한다.
그리고 나서 새로운 행에 놓고자 하는 열/필드 리스트를 명시한다.
그리고 나서 키워드 VALUES와 각 필드 별로 해당하는 값을 넣는다.

SQL SELECT 명령어는 데이터베이스에서 행과 열을 가져오기 위해 사용된다.
SELECT 명령문은 가져오고자 하는 행과 WHERE절을 사용하여 어느 행을 가져올지 지정한다.
선택 사항으로 ORDER BY 절을 이용하여 반환되는 행을 정렬할 수도 있다.




* 을 사용하여 WHERE 절에 매칭되는 각 행의 모든 열을 데이터베이스에서 가져온다.

주목할 점은 파이썬과 달리 SQL WHERE 절은 등식을 시험하기 위해서 두개의 등치 기호 대신에 단일 등치 기호를 사용한다.
WHERE에서 인정되는 다른 논리 연산자는 
<,
>,
<=,
>=,
!= 이고, 논리 표현식을 생성하는데 AND, OR, 괄호를 사용한다.

다음과 같이 반환되는 행이 필드값 중 하나에 따라 정렬할 수도 있다.




행을 제거하기 위해서, SQL DELETE 문장에 WHERE 절이 필요하다.
WHERE 절이 어느 행을 삭제할지 결정한다.




다음과 같이 SQL UPDATE 문장을 사용해서 테이블에 하나 이상의 행 내에 있는 하나 이상의 열을 갱신(UPDATE)할 수 있다.




UPDATE 문장은 먼저 테이블을 명시한다.
그리고 나서, SET 키워드 다음에 변경할 필드 리스트 와 값을 명시한다.
그리고 선택사항으로 갱신될 행을 WHERE절에 지정한다. 
단일 UPDATE 문장은 WHERE절에서 매칭되는 모든 행을 갱신한다. 
혹은 만약 WHERE절이 지정되지 않으면,테이블 모든 행에 대해서 갱신(UPDATE)을 한다.

네가지 기본 SQL 명령문(INSERT, SELECT, UPDATE, DELETE)은 데이터를 생성하고 유지 관리하는데 필요한 기본적인 4가지 작업을 가능케 한다.


데이터베이스를 사용한 트위터 스파이더링(Spidering)

이번장에서 트위커 계정을 조사하고 데이터베이스를 생성하는 간단한 스파이더링 프로그램을 작성합니다.
주의: 프로그램을 실행할 때 매우 주의하세요. 여러분의 트위터 계정 접속이 차단될 정도로 너무 많은 데이터를 가져오거나 
장시간 프로그램을 실행하지 마세요.

임의 스파이더링 프로그램이 가지는 문제점 중의 하나는 종종 중단되거나 여러번 재시작할 필요가 생긴다는 것이다.
이로 사유로 지금까지 가져온 데이터를 잃을 수도 있다는 것이다.
데이터 가져오기온 처음 시점에서 항상 다시 시작하고 싶지는 않다.
그래서 데이터를 가져오면 저장하길 원한다.
프로그램이 자동으로 백업작업을 수행해서 중단된 곳에서부터 다시 가져오는 작업을 했으면 한다.

출발은 한 사람의 트위터 친구와 상태 정보를 가져오는 것에서 시작한다.
그리고, 친구 리스트를 반복하고, 향후에 가져올 수 있도록 친구 각각을 데이터베이스에 추가한다.
한 사람의 트위터 친구를 처리한 후에, 테이터베이스를 확인하고 친구의 친구 한명을 가져온다. 
이것을 반복적으로 수행하고, "방문하지 않는(unvisited)" 친구를 선택하고,친구의 리스트를 가져온다.
그리고 향후 방문을 위해서 현재 리스트에서 보지 않은 친구를 추가한다.

"인기도(popularity)"를 측정하도록 데이터베이스에 특정 친구를 얼마나 자주 봤는지를 기록한다.

알고 있는 계정 리스트를 저장함으로써, 혹은 계정을 가져왔는 혹은 그렇지 않은지, 그리고 계정이 컴퓨터 하드디스크 데이터베이스에서 얼마나 인기있는지에 따라
원하는 만큼 프로그램을 멈추거나 다시 시작할 수 있다.


프로그램이 약간 복잡하다. 
트위터 API를 사용한 책의 앞선 예제에서 가져온 코드에 기반하여 작성되었다.

다음이 트위터 스파이더링 응용프로그램 소스코드다.




데이터베이스는 spider.sqlite3 파일에 저장되어 있다. 
테이블 이름은 Twitter다.
Twitter 테이블은 계정 이름에 대한 열,
계정 친구 정보를 가져왔는지 여부를 나타내는 열, 
그리고 계정이 얼마나 많이 "친구추가(friended)" 되었는가 나타내는 열로 구성되었다.

프로그램의 메인 루프에서, 사용자가 트위터 계정 이름을 입력하거나 프로그램에서 나가기 위해 "끝내기(quit)"를 입력한다.
사용자가 트위터 계정을 입력하면, 친구 리스트와 상태정보도 가져온다. 
만약 데이터베이스에 존재하지 않다면 데이터베이스에 친구로 추가한다.
만약 친구가 이미 리스트에 존재한다면, 데이터베이스 행으로 friends 필드에 추가한다.

만약 사용자가 엔터키를 누르면, 아직 가져오지 않은 다음 트위터 계정에 대해서 데이터베이스 정보를 살펴본다.
그 계정의 친구와 상태 정보를 가져오고, 데이터베이스에 추가하거나 갱신하고, friends count를 증가한다.

친구 리스트와 상태정보를 가져왔으면, 반환된 JSON 형식 user 항목을 반복돌려 각 사용자의 screen_name을 가져온다. 
그리고 나서 SELECT 문을 사용하여 데이터베이스에 screen_name이 저장되었는지, 레코드가 존재하면 친구 숫자(friends)를 확인한다.




커서가 SELECT문을 수행하면 행을 가져온다. 
for문으로 동일한 작업을 할 수 있지만, 단지 하나의 행(LIMIT 1)만을 가져오기 때문에,
SELECT 처리 결과에서 첫번째만 가져오는 fetchone() 메쏘드를 사용한다.
fetchone()은 설사 하나의 필드만 있더라도 행을 튜플(tuple)로 반환하기 때문에,
[0]을 사용해서 튜플로부터 첫번째 값을 얻어 count 변수에 현재 친구 숫자를 구한다.

정상적으로 데이터를 가져오면, SQL WHERE절을 가진 UPDATE문을 사용하여
친구의 계정에 매칭되는 행에 대해서 friends 열에 추가한다.
SQL에 두개의 플레이스홀더(placeholder, 물음표)가 있고, execute()의 두 매개변수가 
물음표 자리에 SQL 안으로 치환될 값을 가진 두-요소 튜플이 된다.

만약 try 블록에서 코드가 작동하지 않는다면, 아마도 SELECT 문의 WHERE name = ? 절에서 매칭되는 레코드가 없기 때문이다.
그래서, except 블록에서, SQL INSERT문을 사용하여 screen_name을 가져온 적이 없고 친구 숫자를 0 으로 설정해서 
친구의 screen_name을 테이블에 추가한다. 

처음 프로그램을 실행하고 트위터 계정을 입력하면, 프로그램이 다음과 같이 실행된다.




프로그램을 처음으로 실행하여서, 데이터베이스는 비여 있다. 
spider.sqlite3 파일에 데이터베이스를 생성하고, Twitter 이름의 테이블을 추가한다.
그리고 나서 친구 몇명을 가져온다.
데이터베이스가 비여있기 때문에 모든 친구를 추가한다.

이 지점에서 spider.sqlite3 파일에 무엇이 있는지를 살펴보기 위해서 간단한 데이터베이스 덤퍼(dumper)를 작성한다.




상기 프로그램은 데이터베이스를 열고 Twitter 테이블의 모든 행과 열을 선택하고 
루프를 모든 행에 대해 돌려 행별로 출력한다.

앞서 작성한 트위터 스파이더를 실행한 후에 이 프로그램을 실행하면, 출력 결과는 다음과 같다.




각 screen_name에 대해 한 행만 있다. 
screen_name 데이터를 가져오지 않아서 데이터베이스에 있는 모든 사람은 친구가 한명 뿐이다.

이제 데이터베이스가 트위터 계정 (drchuck)에서 친구 정보를 가져온 것을 확인했다.
프로그램을 반복적으로 실행해서, 
다음과 같이 트위터 계정을 입력하는 대신에 엔터키를 눌러 "처리되지 않은" 다음 계정 친구정보를 가져오게 한다.




엔터키를 누를 때(즉, 트위터 계정을 명시하지 않았을 때), 다음 코드가 수행된다.




SQL SELECT문을 사용해서 첫 사용자(LIMIT 1) 이름을 가져온다.
"사용자를 가져왔는가"의 값은 여전히 0으로 설정되어 있다.
try/except 블록 내부에 fetchone()[0] 패턴을 사용하여 가져온 데이터에서 screen_name을 
추출하던가 혹은 오류 메시지를 출력하고 다시 돌아간다.

처리되지 않은 screen_name을 성공적으로 가져오면, 다음과 같이 데이터를 가져온다.




데이터를 성공적으로 가져오면 UPDATE문을 사용하여 이 계정의 친구 가져오기를 완료했는지 표기하기 위해서 retrieved 열에 1 을 표시한다.
이렇게 함으로써 반복적으로 동일한 데이터를 가져오지 않게 하고 트위터 친구 네트워크를 타고 앞으로 나갈 수 있게 한다.

친구 프로그램을 실행하고 다음 방문하지 않은 친구의 친구 정보를 가져오기 위해서
두번 엔터를 누르고, 결과값을 확인하는 프로그램을 실행하면, 다음 출력값을 얻게 된다.




lhawthorn과 opencontent을 방문한 이력이 잘 기록됨을 볼 수 있다.
cnxorg과 kthanos 계정은 이미 두 명의 팔로워(follower)가 있다.
친구 세명(drchuck, opencontent, lhawthorn)을 가져와서, 테이블은 55 친구 행이 생겼다.

매번 프로그램을 실행하고 엔터키를 누를 때마다, 
다음 방문하지 않은(예, 다음 계정은 steve_coppin) 계정을 선택해서, 
친구 목록을 가져오고, 가져온 것으로 표기하고, steve_coppin 친구 각각을 데이터베이스 끝에
추가하고 데이터베이스에 이미 추가되어 있으면 친구 숫자를 갱신한다.

프로그램의 데이터가 모두 데이터베이스 디스크에 저장되어서, 스파이더링을 잠시 보류할 수 있ek.
데이터 손실 없이 원하는만큼 다시 시작할 수 있다.


데이터 모델링 기초

관계형 데이터베이스의 진정한 힘은 다중 테이블과 테이블 사이의 관계를 생성할 때 생긴다.
응용프로그램 데이터를 쪼개서 다중 테이블과 두 테이블 간에 관계를 설정하는 것을 데이터 모델링(data modeling)이라고 한다. 
테이블 정보와 테이블 관계를 표현하는 설계 문서를 데이터 모델(data model)이라고 한다.

데이터 모델링(data modeling)은 상대적으로 고급 기술이여서 이번 장에서는 관계형 데이터 모델링의 가장 기본적인 개념만을 소개한다.
데이터 모델링에 대한 좀더 자세한 사항은 다음 링크에서 시작해 볼 수 있다.

http://en.wikipedia.org/wiki/Relational_model

트위터 스파이더 응용프로그램으로 단순히 한 사람의 친구가  몇명인지 세는 대신에, 
모든 관계 리스트를 가지고서 특정 계정에 팔로잉하는 모든 사람을 찾는다.

모두 팔로잉하는 계정을 많이 가지고 있어서, 트위터(Twitter) 테이블에 단순히 하나의 열만을 추가해서는 해결할 수 없다.
그래서 친구를 짝으로 추적할 수 있는 새로운 테이블을 생성한다. 
다음이 간단하게 상기 테이블을 생성하는 방식이다.




drchuck을 팔로잉하는 사람을 마주칠 때마다, 다음과 같은 형식의 행을 삽입한다.




drchuck 트위터 피드에서 친구 20명을 처리하면서, 
"drchuck"을 첫 매개변수로 가지는 20개 레코드를 삽입해서 데이터베이스에 중복되는 많은 문자열이 생길 것이다.

문자열 데이터 중복은 데이터베이스 정규화(database normalization) 모범 사례(berst practice)를 위반하게 만든다.
기본적으로 데이터베이스 정규화는 데이터베이스에 결코 한번 이상 동일한 문자열을 저장하지 않는다. 
만약 한번 이상 데이터가 필요하다면, 그 특정 데이터에 대한 숫자 키(key)를 생성하고, 그 키를 사용하여 실제 데이터를 참조한다.

실무에서, 문자열이 컴퓨터 주기억장치나 디스크에 저장되는 정수형 자료보다 훨씬 많은 공간을 차지하고 더 많은 처리시간이 비교나 정렬에 소요된다.
항목이 단지 수백개라면, 저장소나 처리 시간이 그다지 문제되지 않는다. 
하지만, 데이터베이스에 수백만명의 사람 정보와 1억건 이상의 링크가 있다면, 가능한 빨리 데이터를 스캔하는 것이 정말 중요하다.

앞선 예제에서 사용된 Twitter 테이블 대신에 People로 명명된 테이블에 트위커 계정을 저장한다.
People 테이블은 트위터 사용자에 대한 행과 관련된 숫자키를 저장하는 추가 열(column)이 있다.
SQLite는 데이터 열의 특별한  자료형(INTEGER PRIMARY KEY)을 이용하여 테이블에 삽입할 임의 행에 대해서 자동적으로 키값을 추가하는 기능이 있다.

다음과 같이 추가적인 id 열을 가진 People 테이블을 생성할 수 있다.




People 테이블의 각 행에서 친구 숫자를 더 이상 유지관리하고 있지 않음을 주목하세요.
id 열 자료형으로 INTEGER PRIMARY KEY 선택할 때 함축되는 의미는 다음과 같다., 
사용자가 삽입하는 각 행에 대해서  SQLite가 자동으로 유일한 숫자 키를 할당하고 관리하게 한다.
UNIQUE 키워드를 추가해서 SQLite에 name에 동일한 값을 가진 두 행을 삽입하지 못하게 한다.

상기 Pals 테이블을 생성하는 대신에, 데이터베이스에 from_id, to_id 두 정수 자료형 열을 지닌 Follows 테이블을 생성한다.
Follows 테이블은 from_id과 to_id의 조합으로 테이블이 유일하다는 제약사항도 가진다. (즉, 중복된 행을 삽입할 수 없다.)




테이블에 UNIQUE절을 추가한다는 의미는 레코드를 삽입할 때 데이터베이스에서 지켜야하는 규칙 집합을 의사소통하는 것이다.
잠시 후에 보겠지만, 프로그램상에 편리하게 이러한 규칙을 생성한다.
이러한 규칙 집합은 실수를 방지하게 하고 코드를 작성을 간결하게 한다.

본질적으로 Follows 테이블을 생성할 때, "관계(relationship)"를 모델링하여 한 사람이 다른 사람을 "팔로우(follow)"하고
이것을 (a) 사람이 연결되어 있고, (b) 관계을 방향성이 나타나도록 숫자를 짝지어 표현한다.  



다중 테이블을 가지고 프로그래밍
테이블 두개, 주키(primary key)와 앞서 설명된 참조 키를 사용하여 트위터 스파이더링 프로그램을 다시 작성한다.
다음은 새로운 버젼 프로그램 코드다.




프로그램이 다소 복잡해 보인다. 
하지만, 테이블을 연결하기 위해서 정수형 키를 사용하는 패턴을 보여준다.
기본적인 패턴은 다음과 같다.



주키(primary key)와 제약 사항을 가진 테이블을 생성한다.

사람(즉, 계정 이름)에 대한 논리 키가 필요할 때 사람에 대한 id 값이 필요하다.
사람 정보가 이미 People 테이블에 존재하는지에 따라,
(1) People 테이블에 사람을 찾아서 그 사람에 대한 id 값을 가져오거나,
(2) 사람을 People 테이블에 추가하고 신규로 추가된 행의 id 값을 가져온다.

"팔로우(follow)" 관계를 잡아내는 행을 추가한다.

이들 각각을 순서대로 다룰 것이다.


데이터베이스 테이블의 제약사항

테이블 구조를 설계할 때, 데이터베이스 시스템에 몇 가지 규칙을 설정할 수 있다.
이러한 규칙은 실수를 방지하고 잘못된 데이터가 테이블에 들어가는 것을 막는다.
테이블을 생성할 때:




People 테이블에 name 칼럼이 유일(UNIQUE)함을 나타낸다.
Follows 테이블의 각 행에서 두 숫자 조합은 유일하다는 것도 나타낸다.
하나 이상의 동일한 관계를 추가하는 것 같은 실수를 이러한 제약 사항을 통해서 방지한다.

다음 코드에서 이런 제약사항의 장점을 확인할 수 있다.




INSERT 문에 OR IGNORE 절을 추가해서 만약 특정 INSERT가 
"name이 유일(unique)해야 한다"를 위반하게 되면, 데이터베이스 시스템은 INSERT를 무시한다.
데이터베이스 제약 사항을 안전망으로 사용해서 무언가가 우연히 잘못되지 않게 방지한다.

마찬가지로, 다음 코드는 정확히 동일 Follows관계를 두번 추가하지 않는다.




다시 한번, Follows 행에 대해 지정한 유일한 제약사항을 위반하게 되면 INSERT 시도를 무시하도록 데이터베이스에 지시한다.


레코드를 가져오거나 삽입하기

사용자가 트위터 계정을 입력할 때, 만약 계정이 존재한다면, id 값을 찾아야 한다.
만약 People 테이블에 계정이 존재하지 않는다면, 레코드를 삽입하고 삽입된 행에서 id 값을 얻어와야 한다.

이것은 매우 일반적인 패턴이고, 상기 프로그램에서 두번 수행되었다. 
가져온 트위터 JSON 사용자(user) 노드에서 screen_name을 추출할 때, 친구 계정의 id를 어떻게 찾는지 코드가 보여준다.

시간이 지남에 따라 점점 더 많은 계정이 데이터베이스에 존재할 것 같기 때문에, 
SELECT문을 사용해서 People 레코드가 존재하는지 먼저 확인한다.

try 구문 내에서 모든 것이 정상적으로 잘 작동하면(일반적으로,
문장이 "만약 모든 것이 잘 된다면"으로 시작하면, 코드는 필히 try/except를 필요로 한다.), 
fetchone()을 사용하여 레코드를 가져와서, 반환된 튜플의 첫번째 요소만 읽어오고 friend_id에 저장한다.

만약 SELECT가 실패하면, fetchone()[0] 코드도 실패하고 제어권은 except 블록으로 이관된다.




except 코드에서 끝나게 되면, 행을 찾지 못해서 행을 삽입해야 한다는 의미가 된다.
INSERT OR IGNORE를 사용해서 오류를 피하고 데이터베이스에 진정으로 갱신하기 위해서 commit()을 호출한다. 
데이터베이스에 쓰기가 수행된 후에, 얼마나 많은 행이 영향을 받았는지 확인하기 위해서 cur.rowcount로 확인한다. 
단일 행을 삽입하려고 했는데, 영향을 받은 행의 숫자가 1과 다르다면, 그것은 오류다.

삽입(INSERT)이 성공하면, cur.lastrowid를 살펴보고 
신규로 생성된 행의 id 칼럼에 무슨 값이 대입되었는지 알 수 있다.


친구관계 저장하기

트위터 사용자와 JSON 친구에 대한 키값을 알게되면, 
다음 코드로 두 개의 숫자를 Follows 테이블에 삽입하는 것은 간단하다.




데이터베이스를 생성할 때 유일(unique)한 제약조건과 INSERT문에 OR IGNORE을 추가함으로써 
데이터베이스 스스로 관계를 두번 삽입하는 것을 방지하도록 한 것에 주목한다.

다음에 프로그램의 샘플 실행 결과가 있다.




drchuck 계정으로 시작해서, 프로그램이 자동적으로 다음 두개의 계정을 선택해서 데이터베이스에 추가한다.

다음은 프로그램 수행을 완료한 후에 People과 Follows 테이블에 첫 몇개의 행이다.




People 테이블의 id, name, visited 필드와 Follows 테이블 양 끝에 관계 숫자를 볼 수 있다.
People 테이블에서, 사람 첫 세명을 방문해서, 데이터를 가져온 것을 볼 수 있다.
Follows 테이블의 데이터는 drchuck(사용자 1)이 첫 다섯개 행에 보여진 모든 사람에 대해 친구임을 나타낸다.
이것은 당연한데 왜냐하면 처음 가져와서 저장한 데이터가 drchuck의 트위터 친구들이기 때문이다.
Follows 테이블에서 좀더 많은 행을 출력하면, 사용자 2, 3 혹은 그 이상의 친구를 볼 수 있다.


세 종류의 키

지금까지 데이터를 다중 연결된 테이블에 넣고 키(keys)를 사용하여 행을 연결하는 방식으로 데이터 모델을 생성했는데,
키와 관련된 몇몇 용어를 살펴볼 필요가 있다. 
일반적으로 데이터베이스 모델에서 세가지 종류의 키가 사용된다.



논리 키(logical key)는 "실제 세상"이 행을 찾기 위해서 사용하는 키다.
데이터 모델 예제에서, name 필드는 논리키다. 
사용자에 대해서 screen_name이고, name 필드를 사용하여 프로그램에서 여러번 사용자 행을 찾을 수 있다.
논리 키에 UNIQUE 제약 사항을 추가하는 것이 의미있다는 것을 종종 이해하게 된다.
논리 키는 어떻게 바깥 세상에서 행을 찾는지 다루기 때문에, 테이블에 동일한 값을 가진 다중 행이 존재한다는 것은 의미가 없다.

주키(primary key)는 통상적으로 데이터베이스에서 자동 대입되는 숫자다.
프로그램 밖에서는 일반적으로 의미가 없고, 단지 서로 다른 테이블에서 행을 열결할 때만 사용된다.
테이블에 행을 찾을 때, 통상적으로 주키를 사용해서 행을 찾는 것이 가장 빠르게 행을 찾는 방법이다.
주키는 정수형이어서, 매우 적은 저장공간을 차지하고 매우 빨리 비교 혹은 정렬할 수 있다.
이번에 사용된 데이터 모델에서 id 필드가 주키의 한 예가 된다.

외부 키(foreign key)는 일반적으로 다른 테이블에 연관된 행의 주키를 가리키는 숫자다.
이번에 사용된 데이터 모델의 외부 키의 사례는 from_id다.

주키 id필드명을 호출하고, 항상 외부키에 임의 필드명에 접미사로 _id 붙이는 명명규칙을 사용한다.


JOIN을 사용하여 데이터 가져오기

데이터 정규화 규칙을 따라서, 데이터를 주키와 외부키로 연결된 두개의 테이블로 나누어서,
테이블 데이터를 다시 합치기 위해서 SELECT를 작성할 필요가 있다.

SQL은 JOIN절을 사용해서 테이블을 다시 연결한다. 
JOIN절에서 테이블 사이의 행을 다시 연결할 필드를 지정한다.

다음은 JOIN절을 가진 SELECT 예제이다.




JOIN절은 Follows와 People 테이블에서 선택하는 필드를 나타낸다. 
ON절은 어떻게 두 테이블이 합쳐지는지를 나타낸다.
Follows에서 행을 선택하고 People에서 행을 추가하는데, 
Follows 테이블의 from_id와 People 테이블의 id 값은 동일하다.


JOIN 결과는 People 테이블 필드와 Follows 테이블에서 매칭되는 필드를 가진 "메타-행(meta-row)"을 생성한다.
People 테이블 id 필드와 Follows 테이블 from_id 사이에 하나 이상의 매칭이 존재한다면,
JOIN은 필요하면 데이터를 중복하면서, 행에 매칭되는 짝 각각에 대해 메타-행을 생성한다.

다중 테이블 트위터 프로그램을 수차례 수행한 후에 데이터베이스에 있는 데이터를 가지고 다음 코드가 시연한다.




이 프로그램에서 People과 Follows 테이블을 먼저 보여주고, 함께 JOIN된 데이터 일부분을 보여준다.

다음이 프로그램 출력이다.




People과 Follows 테이블에서 칼럼을 볼 수 있고, 마지막 행의 집합은 JOIN절을 가진 SELECT문의 결과다.

마지막 SELECT문에서, "opencontent" (즉 People.id=2)를 친구로 가진 계정을 찾는다.

마지막 SELECT "메타-행"의 각각에서 첫 두 열은 Follows 테이블에서, 3번째부터 5번째 열은 People 테이블에서 가져왔다. 
두번째 열(Follows.to_id)과 세번째 열(People.id)은 join된 "메타-열"에서 매칭됨을 볼 수 있다.


요약

이번 장은 파이썬에서 데이터베이스 사용 기본적인 개요에 대해 폭넓게 다루었다.
데이터를 저장하기 위해서 파이썬 딕셔너리나 일반적인 파일보다 데이터베이스를 사용하여 코드를 작성하는 것이 훨씬 복잡하다.
그래서, 만약 작성하는 응용프로그램이 실질적으로 데이터베이스 역량을 필요하지 않는다면 굳이 데이터베이스를 사용할 이유는 없다.
데이터베이스가 특히 유용한 상황은 
(1) 큰 데이터셋에서 작은 임의적인 갱신이 많이 필요한 응용프로그램을 작성할 때
(2) 데이터가 너무 커서 딕셔너리에 담을 수 없고 반복적으로 정보를 검색할 때, 
(3) 한번 실행에서 다음 실행 때까지 데이터를 보관하고, 멈추고, 재시작하는데 매우 긴 실행 프로세스를 갖는 경우다.

많은 응용프로그램 요구사항을 충족시키기 위해서 단일 테이블로 간단한 데이터베이스를 구축할 수 있다.
하지만, 대부분의 문제는 몇개의 테이블과 서로 다른 테이블간에 행이 연결된 관계를 요구한다.
테이블 사이 연결을 만들 때, 좀더 사려깊은 설계와 데이터베이스의 역량을 가장 잘 사용할 수 있는 데이터베이스 정규화 규칙을 따르는 것이 중요하다. 
데이터베이스를 사용하는 주요 동기는 처리할 데이터의 양이 많기 때문에, 
데이터를 효과적으로 모델링해서 프로그램이 가능하면 빠르게 실행되게 만드는 것이 중요하다.


디버깅

SQLite 데이터베이스에 연결하는 파이썬 프로그램을 개발할 때 하나의 일반적인 패턴은
파이썬 프로그램을 실행하고 SQLite 데이터베이스 브라우저를 통해서 결과를 확인하는 것이다.
브라우저를 통해서 빠르게 프로그램이 정상적으로 작동하는지를 확인할 수 있다.

SQLite에서 두 프로그램이 동시에 동일한 데이터를 변경하지 못하기 때문에 주의가 필요하다.
예를 들어, 브라우저에서 데이터베이스를 열고 데이터베이스에 변경을 하고 "저장(save)"버튼을 누르지 않는다면,
브라우져는 데이터베이스 파일에 "락(lock)"을 걸구, 다른 프로그램이 파일에 접근하는 것을 막는다.
특히, 파일이 잠겨져 있으면 작성하고 있는 파이썬 프로그램이 파일에 접근할 수 없다.

해결책은 데이터베이스가 잠겨져 있어서 파이썬 코드가 작동하지 않는 문제를 피하도록 
파이썬에서 데이터베이스에 접근하려 시도하기 전에 데이터베이스 브라우져를 닫거나 혹은 File 메뉴를 사용해서 브라우져 데이터베이스를 닫는 것이다.


용어정의



[속성(attribute):] 튜플 내부에 값의 하나. 좀더 일반적으로 "열", "칼럼", "필드"로 불린다.
[제약(constraint):] 
데이터베이스가 테이블의 필드나 행에 규칙을 강제하는 것.
일반적인 제약은 특정 필드에 중복된 값이 없도록 하는 것(즉, 모든 값이 유일해야 한다.)
[커서(cursor):]
커서를 사용해서 데이터베이스에서 SQL 명령어를 수행하고 데이터베이스에서 데이터를 가져온다.
커서는 네트워크 연결을 위한 소켓이나 파일의 파일 핸들러와 유사하다.
[데이터베이스 브라우져(database browser):] 
프로그램을 작성하지 않고 직접적으로 데이터베이스에 연결하거나 데이터베이스를 조작할 수 있는 소프트웨어.
[외부 키(foreign key):]
다른 테이블에 있는 행의 주키를 가리키는 숫자 키.
외부 키는 다른 테이블에 저장된 행사이에 관계를 설정한다.
[인텍스(index):]
테이블에 행이 추가될 때 정보 검색하는 것을 빠르게 하기 위해서 데이터베이스 소프트웨어가 유지관리하는 추가 데이터.
[논리 키(logical key):]
"외부 세계"가 특정 행의 정보를 찾기 위해서 사용하는 키. 
사용자 계정 테이블의 예로,
사람의 전자우편 주소는 사용자 데이터에 대한 논리 키의 좋은 후보자가 될 수 있다.
[정규화(normalization):]
어떠한 데이터도 중복이 없도록 데이터 모델을 설계하는 것.
데이터베이스 한 장소에 데이터 각 항목 정보를 저장하고 외부키를 이용하여 다른 곳에서 참조한다.
[주키(primary key):]
다른 테이블에서 테이블의 한 행을 참조하기 위해서 각 행에 대입되는 숫자 키.
종종 데이터베이스는 행이 삽입될 때 주키를 자동 삽입하도록 설정되었다.
[관계(relation):]
튜플과 속성을 담고 있는 데이터베이스 내부 영역. 
좀더 일반적으로 "테이블(table)"이라고 한다.
[튜플(tuple):]
데이터베이스 테이블에 단일 항목으로 속성 집합이다. 좀더 일반적으로 "행(row)"이라고 한다.





데이터 시각화

지금까지 파이썬 언어 자체를 학습했고, 데이터를 다루기 위해서 파이썬, 네트워크, 그리고 데이터베이스를 어떻게 활용하는가도 학습했다.

이번장에서는 학습한 모두를 모아 완전한 응용프로그램 세개를 작성하여 데이터 관리와 시각화 할 것이다.
실제 문제를 해결하는데 시작할 수 있는 샘플 코드로도 응용프로그램을 사용할 수도 있다.

각 응용프로그램은 ZIP 파일로 압축되어서 다운로드 받아 로컬 컴퓨터에 압축을 풀고 실행한다.


지리정보 데이터로 구글맵 생성하기
이번 프로젝트에서 구글 지오코딩(geocoding) API를 사용해서 사용자가 입력한 대학교 이름 지리 정보를 정리하고 구글 지도에 데이터를 표시한다.


시작하기 위해서 다음 url에서 응용프로그램을 다운로드 한다.

www.py4inf.com/code/geodata.zip

해결할 첫번째 문제는 무료 구글 지오코딩 API가 일일 요청횟수에 제한이 있다는 것이다.
그래서, 만약 데이터가 많다면, 여러번 중지, 재시작하는 검색 프로세스가 요구된다.
그래서, 문제를 두 단계로 나누었다.

첫번째 단계에서, where.data 파일에 "설문(survey)" 데이터를 받는다.
한번에 한줄씩 읽고 구글에서 지리정보를 자져와 geodata.sqlite 데이터베이스에 저장한다.
각 사용자가 입력한 위치에 대한 지오코딩 API를 사용하기 전에, 특정 입력 라인에 데이터가 있는지를 확인하기 위해서 간단히 점검한다.
데이터베이스는 지오 코딩 데이터의 로컬 "캐쉬(cache)"처럼 동작해서 두번 동일 데이터에 대해서는 구글에 요청하지 않도록 한다.

geodata.sqlite 파일을 제거함으로써 언제라도 프로세스를 재시작할 수 있다.

geoload.py 프로그램을 실행한다. 
geoload.py 프로그램은 where.data에 입력 라인을 읽고, 각 라인별로 데이터베이스 존재유무를 확인한다. 
만약 위치에 대한 데이터가 없다면, 지오코딩 API를 호출해서 데이터를 가져오고 데이터베이스에 저장한다.

약간의 데이터가 데이터베이스에 이미 존재한 것을 확인한 후에 샘플로 실행한 결과가 다음에 있다.




첫 다섯 지점은 이미 데이터베이스에 있으므로 건너뛴다. 
프로그램은 가져오지 못한 위치 정보를 찾아 스캔하고 정보를 가져온다.

geoload.py는 언제라도 멈춰 정지시킬 수 있다. 
실행할 때마다 지오코딩 API 호출 횟수를 제한하기 위한 카운터가 있다.
where.data에 수백개의 데이터 항목만 있다면, 하루 사용량 한계를 넘지 않을 것을 것이다. 
하지만, 만약 더 많은 데이터가 있다면, 모든 입력 데이터에 대한 모든 지리정보 데이터로 구성된 데이터베이스를 만들기 위해서 
몇일에 걸쳐서 여러번 실행할지도 모른다.

데이터를 geodata.sqlite에 적재하면, geodump.py 프로그램을 사용하여 데이터를 시각화할 수 있다.
프로그램이 데이터베이스를 읽고 위치, 위도, 경도를 실행가능한 자바스크립 코드로 where.js에 쓴다.

geodump.py 프로그램 실행결과는 다음과 같다.




where.html 파일은 HTML과 자바스크립트로 구성되어서 구글 맵을 시각화한다.
where.js에 가장 최신 데이터를 읽어서 시각화할 데이터로 사용한다.
where.js 파일 형식은 다음과 같다.




리스트의 리스트를 담고 있는 자바스크립트다. 
자바스크립트 리스트 상수에 대한 구문은 파이썬과 매우 유사하여, 구문이 여러분에게 매우 친숙해야 한다.

위치를 보기 위해서 브라워져에서 where.html을 연다. 
각 맵 핀을 여기저기 돌아다니면서 지오코딩 API가 사용자가 입력한 것에 대해서 반환한 위치를 찾는다.
where.html 파일을 열었을 때 어떤 데이터도 볼 수 없다면, 자바스크립트나 브라우저 개발자 콘솔을 확인한다.


네트워크와 상호 연결 시각화
이번 응용프로그램에서 검색엔진 기능 일부를 수행한다.
먼저 웹의 일부분을 스파이더링하고 어느 페이지가 가장 많이 연결되었는지 결정하기 위해서
간략한 구글 페이지 랭크 알고리즘을 실행한다.
스파이더링한 작은 웹 조각에 대해서 연결성과 페이지 랭크를 시각화한다. 
시각화 산출물을 만들기 위해서 D3 자바스크립트 시각화 라이브러리 http://d3js.org/를 사용한다.

응용프로그램을 다음 url에서 다운로드 받아 압축을 푼다.

www.py4inf.com/code/pagerank.zip


첫 프로그램(spider.py)은 웹사이트를 크롤(craw)하고 일련의 페이지를 뽑아내서 데이터베이스(spider.sqlite)에 넣어 페이지 간에 링크를 기록한다. 
언제라도 spider.sqlite  파일을 제거하고 spider.py를 재실행함으로써 프로세스를 다시 시작할 수 있다.




샘플 실행결과, 웹사이트를 크롤해서, 웹페이지 두개를 가져왔다.
프로그램을 재실행해서 좀더 많은 웹페이지를 크롤하게 한다면, 데이터베이스에 이미 등록된 웹페이지는 다시 크롤하지는 않는다.
재시작할 때 무작위로 크롤하지 않은 페이지로 이동해서 그곳에서 크롤링을 시작한다.
그래서 연속적으로 spider.py을 실행하는 것은 추가적이된다.




같은 데이터베이스에서 다중 시작 지점을 가질 수 있다.
프로그램 내부에서 이를 "webs"라고 부른다.
스파이더는 모든 웹사이트에서 방문하지 않는 링크 중에 무작위로 선택하여 방문할 다음 웹 페이지를 선정한다. 

spider.sqlite 파일에 들어있는 내용을 살펴보고자 한다면, 다음과 같이 spdump.py을 실행한다.




인입 링크 수, 이전 페이지 랭크, 신규 페이지 랭크, 페이지 id, 페이지 url을 보여준다.
spdump.py 프로그램은 최소한 하나의 인입 링크가 있는 페이지만을 보여준다.

데이터베이스에 몇개의 페이지가 있다면, sprank.py 프로그램을 실행하여 페이지에 대한 페이지 랭크를 실행한다. 
단순하게 얼마나 많이 페이지 랭크 반복을 수행할지 지정한다.




데이터베이스를 다시 열어서 페이지 랭크가 갱신되었는지 확인한다.




sprank.py를 원하는 만큼 실행한다. 
실행할 때마다 페이지 랭크를 정교화한다.
sprank.py을 몇번 실행하고 나서 spider.py으로 좀더 많은 페이지를 스파이더링한다.
그리고 나서, sprank.py을 실행하여 페이지 랭크 값을 다시 수렴하여 갱신한다. 
검색엔진은 일반적으로 동시에 크롤링과 랭킹 프로그램을 실행한다.

웹페이지를 다시 스파이더링하지 않고, 페이지 랭크 계산을 재시작하고자 한다면,
spreset.py을 사용하고 나서 sprank.py를 다시 시작한다.




페이지 랭크 알고리즘을 매번 반복하면, 페이지 마다 페이지 랭크 평균 변화를 출력한다.
초기에 네트워크는 매우 불균형 상태여서 각각의 페이지 랭크 값은 반복할 때마다 요동친다.
하지만, 짧게 몇번 반복한 다음에 페이지 랭크는 수렴한다.
prank.py을 충분히 오랜동안 실행해서 페이지 랭크 값을 수렴하게 만든다.

페이지 랭크에 대해서 현재 최상위 페이지를 시각화하고자 한다면, 
spjson.py을 실행하여 데이터베이스를 읽고 웹브라우져에서 볼 수 있는 JSON 형식으로 가장 많이 연결된 페이지에 대해서 데이터를 저장한다.




웹브라우져에 force.html 파일을 열어 작업 데이터 결과를 볼 수 있다.
노드와 링크의 자동 레이아웃을 보여준다. 
임의 노드를 클릭하고 끌 수도 있고, 노드를 더블 클릭해서 노드로 표현된 URL을 확인할 수 있다.

만약 다른 유틸리티를 재실행하고자 하면, spjson.py을 다시 실행하고 브라우져의 새로 고치기를 눌러서
spider.json에서 새로운 데이터를 가져온다.



전자우편 데이터 시각화

책을 이 지점까지 읽어오면서 mbox-short.txt과 mbox.txt 파일에 매우 친숙해졌을 것이다.
이제는 전자우편 데이터 분석 수준을 다음 단계로 옮겨갈 때다.

실무에서 종종 서버에서 전자우편 데이터를 가져오는 것은 시간이 많이 걸리고, 가져온 데이터는 오류가 많고, 일관되지 못하고, 많은 보정과 정비가 필요하다.
이번 장에서, 지금까지 작성한 가장 복잡한 응용프로그램을 가지고 거의 1GB 데이터를 추출하여 시각화 작업을 한다.


응용프로그램을 다음 url에서 다운로드한다.

www.py4inf.com/code/gmane.zip

www.gmane.org의 무료 전자우편 보관 서비스 데이터를 사용한다.
전자우편 활동에 대한 멋진 보관 서비스를 검색기능과 함께 제공하기 때문에, 오픈 소스 프로젝트에서는 매우 인기가 높다.
API를 통한 데이터 접근에 대해서도 매우 자유로운 정책을 유지한다.
사용량에 대한 제한은 없지만, 자율적으로 서비스에 부하를 너무 많이 주지 말고 필요한 데이터만 가져가도록 요청한다.
gmane의 사용조건에 대해서는 다음 페이지를 참조한다.

http://gmane.org/export.php

서비스 접근에 지연을 추가하거나 오랜 기간에 걸쳐서 장시간이 소요되는 작업을 분산함으로써
gmane.org 데이터를 책임감있게 사용하는 것은 매우 중요하다. 다른 사용자를 위해서 무료 서비스를 오용하지 말고, 
서비스를 망치지 말아주세요.

이 프로그램을 사용하여 Sakai 전자우편 주소 데이터를 스파이더링하면, 거의 1 GB의 데이터를 생성하고 몇일에 걸쳐서 수차례 실행을 해야한다. 
ZIP 압축파일 내부에 README.txt이 어떻게 대부분의 Sakai 전자우편 코퍼스(corpus)를 다운로드할 수 있는지에 대한 안내정보를 담고 있다.
그래서 단지 프로그램을 실행하기 위해서 5일 동안 스파이더링을 할 필요는 없다. 
이미 스파이더링된 콘텐츠를 다운로드했다면, 좀더 최근의 메시지를 가져오기 위해서 스파이더링 프로세스를 다시 실행해야 한다.

첫 단계는 gmane 저장소를 스파이더링하는 것이다. 
기본 URL은 gmane.py 파일과 Sakai 개발자 리스트에 하드코딩 되어 있다. 
기본 url을 변경함으로써 또다른 저장소를 스파이더링 할 수 있다.
기본 url을 변경하는 경우 content.sqlite 파일을 필히 삭제하세요.

gmane.py 파일은 책임감있는 캐쉬 스파이더로서 작동한다. 
천천히 실행되며 1초에 한개의 전자우편 메시지를 가져와서 gmane에 의해서 작동 못하게 되는 것을 피한다.
데이터베이스 모든 데이터를 저장한다. 
필요하면 자주 중단하고 재시작한다.
모든 데이터를 가져오는데 몇 시간이 걸릴 수 있다. 
그래서 여러번 재시작하는 것이 필요하다. 

Sakai 개발자 리스트에 있는 마지막 5개 메시지를 가져오는 프로그램 gmane.py을 실행한 결과다.




프로그램은 1번 부터 이미 스파이더링되지 않는 첫 메시지까지 content.sqlite을 스캔하고 그 메시지에서 스파이더링을 시작한다. 
원하는 메시지 숫자를 스파이더링할 때까지 계속하거나 잘못된 형식의 메시지가 나오는 페이지에 도달할 때까지 스파이더링을 계속한다.

때때로 gmane.org도 메시지를 잃어버린다. 
아마도 관리자가 메시지를 삭제하거나 아마도 분실했을 것이다. 
만약 스파이더가 멈추고 잃어버린 메시지를 만나게 되면 SQLite 매니저로 가서 다른 모든 행을 공백으로 구성하고 잃어버린 id를 가진 행을 추가하여 gmane.py을 재시작한다. 
이렇게 함으로써 스파이더링 프로세스를 재시작하고 계속 진행할 수 있다.
빈 메시지는 다음번 프로세스에서는 무시된다.

모든 메시지를 스파이더링하고 content.sqlite 파일에 넣게 되면, 
리스트에 메시지가 보내질 때 gmane.py을 다시 시작해서 새로운 메시지를 얻게 된다.

content.sqlite 데이터는 원데이터 형식이고 비효율적인 데이터 모델을 가지고 있으며 압축되어 있지 않다.
의도적으로 그렇게 함으로써 SQLite 매니저가 content.sqlite 파일을 직접 살펴보고, 스파이더링 프로세스에서 문제를 디버그할 수 있도록 한다. 
매우 느린 상태에서 데이터베이스에 질의를 실행하는 것은 좋은 생각은 아니다.

두번째 프로세스는 gmodel.py 프로그램을 실행하는 것이다. 
content.sqlite로부터 가다듬지 않은 원데이터를 읽어서 index.sqlite 파일에 깨끗하게 잘 모델링된 형식 데이터로 저장한다.
index.sqlite 파일은 content.sqlite 보다 10배이상 크기가 적다. 
왜냐하면 헤더와 본문 텍스트를 압축하기 때문이다.

gmodel.py을 실행할 때마다, index.sqlite를 삭제하고 다시 생성한다.
그래서 함으로써 데이터 정비 프로세스를 약간 바꾸는데 매개변수를 조정하고 content.sqlite의 매핑 테이블을 편집할 수 있게 한다.
다음은 gmodel.py을 샘플로 실행시킨 것이다. 
매번 250개 전자우편 메시지가 처리될 때마다 한 라인을 출력해서 거의 1GB 전자우편 데이터를 처리하는 동안 진행사항을 지켜볼 수 있다.




gmodel.py 프로그램은 많은 데이터 정비 작업을 처리한다.

도메인 이름이  .com, .org, .edu, .net에 대해서는 2 단계로 다른 도메인 이름은 3 단계로 잘랐다.
그래서 si.umich.edu은 umich.edu이 되고, caret.cam.ac.uk은 cam.ac.uk이 된다.
또한 전자우편 주소는 소문자가 되게 하고, 다음과 같은  @gmane.org 주소는 
메시지 코퍼스 어딘가 있는 실제 전자우편 주소와 매칭하여 실제 주소로 변환한다.




content.sqlite 데이터베이스를 살펴보면, 도메인 이름과 전자우편 주소가 시간에 따라 변경되는 것을 보정하려고 개인 전자우편 주소를 매핑하는 테이블이 두개 있다.
예를 들어, Sakai 개발자 리스트 개발기간에 걸쳐 직업을 바꿈에 따라 Steve Githens는 다음 전자우편 주소를 사용했다.




content.sqlite의 매핑(Mapping) 테이블에 항목을 두개를 추가해서 gmodel.py 프로그램이 3개 주소를 하나로 매핑한다.




다중 DNS 이름을 하나의 DNS 이름으로 매핑하려면 DNSMapping 테이블에 비슷한 항목을 생성할 수 있다.
다음 매핑이 Sakai 데이터에 추가된다.




그래서, 다양한 인디애나 대학교 캠퍼스의 모든 계정이 함께 추적된다.

데이터를 볼 때마다, gmodel.py를 반복적으로 실행하고 데이터를 좀더 깨끗하게 만들기 위해서 매핑을 추가한다. 
작업이 마쳐지면, index.sqlite에 인덱스 잘된 버젼의 전자우편 데이터가 있다.
자료 분석을 위해 이 파일을 사용한다. 
이 파일로 데이터분석은 정말 빠르게 수행된다.

첫번째, 가장 간단한 자료 분석은 "누가 가장 많은 전자우편을 보냈는가?"와 "어느 조직에서 가장 많은 전자우편을 보냈는가?"를 알아보는 것이다. 
gbasic.py을 사용해서 수행할 수 있다.




gmane.py이나 혹은 gmodel.py과 비교하여 gbasic.py이 얼마나 빨리 실행되는지 주목하세요.
모두 동일한 데이터에서 작업을 수행하지만, gbasic.py은 index.sqlite에 저장된 압축되고 정규화된 데이터를 사용한다. 
처리할 데이터가 많다면, 응용프로그램과 같이 다중-단계 프로세스를 개발하는데 시간이 약간 더 걸리지만, 데이터를 탐색하고 시각화할 때는 많은 시간을 절약해 준다. 

gword.py 파일에 주제 라인의 단어 빈도를 간략히 시각화할 수 있다.




이번 장 처음의 것과 유사한 워드 클라우드를 생성하기 위해서 gword.htm를 사용하여 시각화할 gword.js 파일을 생성한다.

두번째 시각화는 gline.py으로 생성된다. 시간에 따른 조직별 전자우편 참여를 연산한다.




출력값은 gline.htm을 사용하여 시각화된 gline.js에 쓰여진다.


작성한 프로그램은 상대적으로 복잡하고 정교한 응용프로그램으로 실제 데이터를 불러오고, 정비하고, 시각화하는 기능을 갖고 있다.





컴퓨터의 일반적인 작업 자동화

파일, 네트워크, 서비스, 그리고 데이터베이스에서 데이터를 읽어왔다.
파이썬은 또한 여러분의 로컬 컴퓨터 디렉토리와 폴더를 훑어서 파일도 읽어온다.

이번 장에서, 여러분의 로컬 컴퓨터를 스캔하고 각 파일에 대해서 연산을 수행하는 프로그램을 작성한다.
파일은 디렉토리(또한 "폴더"라고도 부른다.)에 정렬되어 보관된다. 
간단한 파이썬 스크립트로 전체 로컬 컴퓨터나 디렉토리 여기저기 뒤져야 찾아지는 수백 수천개 파일에 대한 단순한 작업을 짧게 수행한다. 

트리상의 디렉토리나 파일을 여기저기 돌아다니기 위해서 os.walk과 for 루프를 사용한다.
open이 파일 콘텐츠를 읽는 루프를 작성하는 것과 비슷하게,
socket은 네트워크 연결된 콘텐츠를 읽는 루프를 작성하고,
urllib는 웹문서를 열어 콘텐츠를 루프를 통해서 읽어오게 한다. 


파일 이름과 경로
모든 실행 프로그램은 "현재 디렉토리(current directory)"를 가지고 있는데 작업 대부분을 수행하는 디폴트 디렉토리가 된다. 
예를 들어, 읽기 위해서 파일을 연다면, 파이썬은 현재 디렉토리에서 파일을 찾는다.

os 모듈(os는 "운영체제(operating system)"의 약자)은 파일과 디렉토리를 작업하는 함수를 제공한다.
os.getcwd은 현재 디렉토리 이름을 반환한다.




cwd 는 current working directory의 약자로 현재 작업 디렉토리다.
예제의 결과는 /Users/csev인데 csev 사용자의 홈 디렉토리가 된다.

파일을 식별하는 cwd 같은 문자열을 경로(path)라고 부른다.
상대경로(relative path)는 현재 디렉토리에서 시작하고,
절대경로(absolute path)는 파일 시스템의 가장 최상단의 디렉토리에서 시작한다.

지금까지 살펴본 경로는 간단한 파일 이름이여서, 현재 디렉토리에서 상대적이다.
파일의 절대 경로를 알아내기 위해서 os.path.abspath을 사용한다.




os.path.exists은 파일이나 디렉토리가 존재하는지 검사한다.




만약 존재하면, os.path.isdir이 디렉토리인지 검사한다.




마찬가지로 os.path.isfile은 파일인지를 검사한다.

os.listdir은 주어진 디렉토리에 파일 리스트(그리고 다른 디렉토리)를 반환한다.






예제: 사진 디렉토리 정리하기

얼마 전에 핸드폰에서 사진을 받아서 서버에 저장하는 플릭커(Flickr)와 유사한 소프트웨어를 개발했다.
플릭커가 존재하기 전에 작성해고, 플맄커가 생긴 이후에도 계속해서 사용했다.
왜냐하면 영원히 원본을 보관하고 싶어서다.

문자메시지를 사용해서 한줄 텍스트 문자나 전자우편 주소 제목줄도 보낼 수 있다. 
사진 파일과 마찬가지로 텍스프 파일형식 메시지를 동일한 디렉토리에 저장했다.
사진을 찍은 월, 년, 일, 그리고 시간에 기초한 디렉토리 구조다.
다음은 사진 한장과 설명 텍스트를 가진 예제다.




7년이 지난 후에, 정말 많은 사진과 짧은 설명문이 생겼다. 
시간이 지남에 따라 핸드폰을 바꿨다.
메시지에서 짧은 설명문을 뽑아내는 코드가 잘 동작하지 않고 짧은 설명문 대신에 쓸모없는 데이터를 서버에 추가했다.

파일을 훑어서 어느 텍스트 파일이 정말 짧은 설명문이고, 어는 것이 쓰레기인지 찾아서 잘못된 파일은 삭제하고 싶었다. 
첫번째 할 일은 다음 프로그램을 사용하여 폴더에 전체 텍스트 파일 목록을 얻는 것이다.




이것을 가능하게 하는 가장 중요한 코드는 파이썬 os.walk 라이브러리다.
os.walk을 호출하고 시작 디렉토리를 지정해 주면, 재귀적으로 모든 디렉토리와 하위 디렉토리를 훑는다.
"." 문자열은 현재 디렉토리에서 시작해서 하위 디렉토리로 훑는 것을 표시한다.
디렉토리 각각에 도착하면, for 루프 몸통부분에 있는 튜플에서 값 세개를 얻는다.
첫번째 값은 현재 디렉토리 이름, 두번째 값은 현재 디렉토리의 하위 디렉토리 리스트, 그리고 세번째 값은
현재 디렉토리 파일 리스트다.

명시적으로 하위 디렉토리 각각을 살펴보지 않는다. 
왜냐하면, os.walk가 자동으로 모든 폴더를 방문할 것이기 때문이다.
하지만, 각 파일을 살펴보고 싶기 때문에, 간단한 for 루프를 작성해서 현재 디렉토리에 
파일 각각을 조사한다. 
".txt"로 끝나는 파일이 있는지 확인한다.
전체 디렉토리 트리를 훑어서 접미사 ".txt"로 끝나는 파일 숫자를 카운트한다.

얼마나 많은 파일이 ".txt" 확장자로 끝나는지 감을 잡았으면, 다음 작업은 자동적으로
어느 파일이 정상이고, 어느 파일이 문제가 있는지를 파이썬이 결정하도록 하는 것이다.
간단한 프로그램을 작성해서 파일과 파일의 크기를 출력한다.




파일을 단순히 카운트하는 대신에 os.path.join을 사용하여 디렉토리 안에서 
디렉토리 이름과 파일 이름을 합쳐서 파일 이름을 생성한다.
문자열 연결 대신에 os.path.join을 사용하는 것이 중요한데 왜냐하면
파일 경로를 나타내기 위해서 윈도우에서는 파일 경로를 생성하기 위해서 역슬래쉬(\)를 사용하고,
리눅스나 애플에서는 슬래쉬 (/)를 사용하기 때문이다.
os.path.join은 이러한 차이를 알고 어느 운영체제에서 동작하는지 인지하고 시스템에 따라
적절한 합치기 작업을 수행한다. 
그래서 동일한 파이썬 코드가 윈도우나 유닉스계열 시스템에도 실행된다.

디렉토리 경로를 가진 전체 파일 이름을 갖게 되면, os.path.getsize 유틸리티를 사용해서
크기를 얻고 출력해서 다음 결과값을 만들어 낸다.





출력값을 스캔하면, 몇몇 파일은 매우 짧고, 다른 많은 파일은 매우 큰데 동일한 크기(2578, 2565)임을 볼 수 있다.
수작업으로 몇개의 큰 파일을 살펴보면, 
T-Mobile 핸드폰에서 보내지는 전자우편에 함께 오는 일반적인 동일한 HTML을 가진 것임을 알 수 있다.  




파일을 대충 살펴보면, 파일에 그다지 유용한 정보가 없으므로 삭제한다.

하지만, 파일을 삭제하기 전에, 한 줄 이상인 파일을 찾고 내용을 출력하는 프로그램을 작성한다.
2578 혹은 2565 문자길이를 가진 파일을 보여주지는 않는다. 
왜냐하면, 파일에 더 이상 유용한 정보가 없음을 알기 때문이다.

그래서 다음과 같이 프로그램을 작성한다.




continue를 사용하여 두 "잘못된 크기(bad sizes)" 파일을 건너뛰고, 
나머지 파일을 열고, 파이썬 리스트에 파일 라인을 읽는다.
만약 파일이 하나 이상의 라인이면, 파일에 얼마나 많은 라인이 있는지 출력하고,
첫 세줄을 출력한다.

두개의 잘못된 파일 크기를 제외하고, 모든 한줄짜리 파일이 정상적이라고 가정하고나면
깨끗하게 정리된 파일을 생성된다.




하지만, 한가지 더 성가신 패턴의 파일이 있다. 두 공백 라인과 "Sent from my iPhone"으로 구성된 3줄짜리 파일이다.
프로그램을 다음과 같이 변경하여 이러한 파일도 처리하게 한다.




단순하게 3줄짜리 파일이 있는지 검사하고 만약 지정된 텍스트로 세번째 라인이 시작한다면, 건너뛴다.

이제 프로그램을 실행하면, 4개의 다중 라인 파일만을 보게되고 모든 파일이 잘 처리된 것으로 보인다.




프로그램의 전반적인 패턴을 살펴보면, 연속적으로 파일을 어떻게 승인할지와 거절할지를 정교화했고,
"잘못된" 패턴을 발견하면 continue를 사용해서 잘못된 파일을 건너뛰게 했다.
그래서 잘못된 더 많은 파일 패턴이 탐지되도록 코드를 정교화했다.

이제 파일을 삭제할 준비가 되었다. 로직을 바꿔서, 나머지 올바른 파일을 출력하는 대신에,
삭제할 "잘못된" 파일만을 출력한다.




이제 삭제할 대상 목록과 왜 이 파일이 삭제 대상으로 나왔는지를 볼 수 있다.
프로그램은 다음 출력을 생성한다.




무작위로 파일을 검사해서 프로그램에 버그가 우연하게 들어가 있는지 혹은 원치 않는 파일이 
작성한 프로그램 로직에 끌려들어 갔는지 확인할 수 있다.

결과값에 만족하고, 다음이 삭제할 파일 목록임으로, 프로그램에 다음과 같은 변경을 한다.




이번 버젼의 프로그램에서 파일을 출력하고 os.remove을 사용하여 잘못된 파일을 삭제한다.




재미로, 프로그램을 두번 실행하게 되면 모든 잘못된 파일이 삭제되어서 출력값이 없다.

txtcount.py을 다시 실행하면, 899 잘못된 파일이 삭제되었음을 알 수 있다.




이번 장에서 일련의 절차에 따라 파이썬을 사용하여 디렉토리와 파일을 검색하는 패턴을 살펴봤다.
천천히 파이썬을 사용해서 디렉토리를 정리하기 위해서 무엇을 할지를 결정했다.
어느 파일이 좋고 어느 파일이 유용하지 않는지 파악한 후에 파이썬을 사용해서 파일을 삭제하고 
파일 정리를 수행했다.

해결하려고 하는 문제가 무척 간단하여 파일 이름만을 살펴보는 것으로 충분히 처리할 수 있다. 
혹은 모든 파일을 읽고 파일 내부에 패턴을 찾을 필요가 있을 수도 있다.
때때로, 모든 파일을 읽고, 파일의 일부에 변경이 필요할지도 모른다. 
os.walk와 다른 os 유틸리티가 어떻게 사용되는지 이해하기만 하면 이 모든 것은 매우 명확한다.



명령 줄 인자

앞선 장에서 raw_input을 사용하여 파일명을 사용자로부터 입력받고,
파일에서 데이터를 읽어 다음과 같이 처리했다.




파이썬을 시작할 때, 명령 라인에서 파일 이름을 입력 받음으써 프로그램을 간략화 할 수 있다.
지금까지 파이썬 프로그램을 단순하게 실행하고 다음과 같이 명령어 프롬프트에 응답했다.




파이썬 파일 다음에 추가 문자열을 배치하고 파이썬 프로그램에서 
명령 줄 인수(command line arguments)에 접근한다.
명령 줄에서 인자를 읽는 것을 시연하는 간단한 프로그램이 다음에 있다.



sys.argv의 콘텐츠는 문자열 리스트로 첫 문자열은 파이썬 프로그램 이름,
그리고 나머지 문자열은 파이썬 파일 다음의 명령 줄의 인자들이다.

다음은 명령 줄에서 명령 줄 인자 몇개를 읽어들이는 프로그램이다. 




세가지 요소 리스트로 프로그램에 넘겨지는 인자가 3개 있다.
리스트의 첫 요소는 파일 이름 (argtest.py) 그리고 나머지는 파일 이름 뒤에 2 명령 줄 인자다.

파일을 읽어 오는 프로그램을 다시 작성할 수 있다. 
다음과 같이 명령 줄 인자로 파일 명을 받는다. 




두번째 명령 줄 인자를 파일 이름으로 취한다. ( [0] 항목 프로그램 이름은 생략한다.)
파일을 열고 다음과 같이 콘텐츠를 읽는다.




입력값으로 명령문 인자를 사용하는 것은 파이썬 프로그램을 재사용하기 쉽게 하고,
특히 하나 혹은 두개의 문자열을 입력받을 때 유용한다.


파이프(Pipes)

대부분의 운영 시스템은 쉘(shell)로 알려진 명령어 기반 인터페이스를 지원한다.
일반적으로 쉘은 파일 시스템을 탐색하거나 응용 프로그램을 실행하는 명령어를 지원한다.
예를 들어, 유닉스에서 cd 명령어로 디렉토리를 변경하고 ls 명령어로 디렉토리의 콘텐츠를 보여주고,
firefox를 타이핑해서 웹브라우져를 실행한다.

쉘에서 실행시킬 수 있는 어떤 프로그램이나 파이프(pipe)를 사용하여 파이썬에서도 실행시킬 수 있다.
파이프는 작동중인 프로세스를 표현하는 객체다.

예를 들어, 유닉스 명령어( 파이프를 사용하여 ls 같은 운영 시스템 명령어로 대화할 때,
무슨 운영 시스템을 사용하는지 알고 운영 시스템에서 지원되는 명령어로 파이프를 열수 있다는 것이 중요하다.)
ls -l는 정상적으로 현재 디렉토리의 콘텐츠(긴 형식으로)를 보여준다.
os.popen를 가지고 ls 를 실행시킬 수 있다.




인자는 쉘 명령어를 포함하는 문자열이다. 
반환 값은 파일 포인터로 열린 파일처럼 동작한다.
readline으로 한번에 한 라인씩 ls 프로세스로부터 출력을 읽거나 
read로 한번에 전체를 가져올 수 있다.




모두 완료되면, 파일처럼 파이프를 닫는다.




반환되는 값은 ls 프로세스의 최종 상태값이다. 
None은 (오류 없이) 정상적으로 종료됨을 의미한다.


용어정의



[절대경로(absolute path):]
파일이나 디렉토리가 어디에 저장되어 있는지를 저장하는 문자열로 "최상단의 디렉토리"에서 시작해서,
현재 작업 디렉토리에 관계없이 파일이나 디렉토리를 접근하는데 사용할 수 있다.
[체크썸(checksum):] 해싱(hashing)을 참조하세요.
"체크썸(checksum)"단어는 네트워크로 데이터가 보내지거나 백업 매체에 쓰여지고 다시 읽어올 때, 데이터가 왜곡되었는지를
검증하는 필요에서 생겨났다.
데이터가 쓰여지거나 보내질 때, 송신 시스템은 체크썸을 계산하고 또한 체크썸도 보낸다.
데이터가 읽혀지거나 받았을 때, 수신 시스템을 수신된 데이터의 체크썸을 다시 계산하고 받은 체크썸과 비교한다.
만약 체크썸이 매칭되지 않으면, 전송 시에 데이터가 왜곡된 것으로 판단해야 한다.
[명령 줄 인자(command line argument):]파이썬 파일 이름 뒤에 명령 줄에 매개 변수.

[현재 작업 디렉토리(current working directory):]
여러분이 "작업하고 있는" 현재 디렉토리. 
명령-줄 인터페이스에서 대부분의 시스템에 cd 명령어를 사용하여 작업 디렉토리를 변경할 수 있다. 
경로 정보 없이 파일만을 사용하여 파이썬에서 파일을 열게 될 때,
파일은 프로그램을 실행하고 있는현재 작업 디렉토리에 있어야 한다. 

[해싱(hashing):] 
가능한 큰 데이터를 읽고 그 데이터에 대해서 유일한 체크썸을 생성하는 것.
최고의 해쉬 함수는 거의 "충돌(collision)"을 만들지 않는다. 여기서 충돌은 
서로 다른 두 데이터 스트림에 해쉬 함수를 줄 때 동일한 해쉬값을 돌려받는 것이다.
MD5, SHA1, SHA256 는 가장 많이 사용되는 해쉬 함수의 사례다.
[파이프(pipe):]
파이프는 실행하는 프로그램에 연결이다. 
파이프를 사용해서, 데이터를 다른 프로그램에 보내거나 그 프로그램에서
데이터를 받는 프로그램을 작성할 수 있다.
파이프는 소켓(socket)과 매우 유사하다. 
차이점은 파이프는 동일한 컴퓨터에서 실행되는 프로그램을 연결하는데만 사용된다는 것이다. 
(즉, 네트워크를 통해서는 사용할 수 없다.)
[상대경로(relative path):]
파일 혹은 디렉토리가 어디에 저장되었는지를 현재 작업 디렉토리에 상대적으로 표현하는 문자열.
[쉘(shell):]
운영 시스템에 명령줄 인터페이스. 다른 시스템에서는 또한 "터미널 프로그램(terminal program)"이라고 부른다.
이런 인터페이스에서 라인에 명령어와 매개 변수를 타입하고 명령을 실행하기 위해서 "엔터(enter)"를 누른다.
[워크(walk):]
모든 디렉토리를 방문할 때까지 디렉토리, 하위 디렉토리, 하위의 하위 디렉토리 전체 트리를 방문하는 개념을 나타내기 위해서 사용된 용어.
여기서 이것을 "디렉토리 트리를 워크"한다고 부른다.

연습문제


MP3 파일이 대규모로 수집되어 있은 곳에는 같은 노래의 복사본 하나 이상이 다른 디렉토리 혹은 다른 파일 이름으로 저장되어 있을 수 있다.
이번 연습문제의 목표는 이런 중복 파일을 찾는 것이다.



.mp3같은 확장자를 가진 파일을 모든 디렉토리와 하위 디렉토리를 검색해서 동일한 크기를 가진 파일짝을 목록으로 보여주는 프로그램을 작성하세요.
힌트: 딕셔너리를 사용하세요. 
딕셔너리의 키는 os.path.getsize에서 파일의 크기가 되고, 
딕셔너리의 값(value)는 파일 이름과 결합된 경로명이 된다. 
파일을 매번 마주칠 때마다 현재 파일과 동일한 크기를 가진 파일이 이미 존재하는지를 검사한다.
만약 그렇다면, 중복된 크기 파일이 있고 파일 크기와 두 파일 이름을 출력한다. (해쉬에서 한 파일, 찾고 있는 곳에서 또다른 파일)

체크썸(checksum) 알고리즘이나 해싱을 사용하여 중복 콘텐츠를 가진 파일을 찾는 이전의 프로그램을 개작하세요.
예를 들어, MD5 (Message-Digest algorithm 5)는 임의적으로 긴 "메시지"를 가지고 128비트 "체크썸"을 반환한다.
다른 콘텐츠를 가진 두 파일이 같은 체크썸을 반환할 확률은 매우 적다.

wikipedia.org/wiki/Md5에서 MD5에 대해서 더 배울 수 있다. 
다음 코드 조각은 파일을 열고, 읽고, 체크썸을 계산한다.




딕셔너리를 생성해서 체크썸이 키가 되고  파일 이름이 값(value)이 된다.
체크섬을 계산하고 키로 이미 딕셔너리에 있게 되면, 중복 콘텐츠인 두 파일 있어서
딕셔너리에 파일과 방금전에 읽은 파일을 출력한다. 
사진 파일 폴더에서 실행한 샘플 출력물이 다음에 있다.




명백하게 때때로 같은 사진을 한번 이상 보내거나 원본을 삭제하지 않고 종종 사진 사본을 만든다.





윈도우 상에서 파이썬 프로그래밍

이번 부록에서 일련의 단계를 거쳐서 윈도우상에서 파이썬을 실행한다.

파이썬 프로그램을 편집하고 실행하는데취할 수 있는 서로 다른 많은 접근방법이 있다. 이것은 간단한 접근 방법중의 하나다.

먼저, 프로그래머 편집기를 설치할 필요가 있다.
Notepad나 윈도우 워드를 가지고 파이썬 프로그램을 편집할 필요는 없다.
프로그램은 "일반 텍스트(flat-text)" 파일이여서 텍스트 파일을 편집하는데 좋은 에디터만 필요하다.

윈도우 시스템에 추천하고 싶은 편집기는 다음에서 다운받아 설치할 수 있는 Notepad++다.

http://sourceforge.net/projects/notepad-plus/files/

www.python.org 웹사이트에서 파이썬 2를 다운로드한다.

http://www.python.org/download/releases/2.7.5/

파이썬을 설치하면, 컴퓨터에 C:Python27 같은 새로운 폴더가 생긴다.

파이썬 프로그램을 생성하기 위해서 시작 메뉴에서 NotePad++를 실행하고 확장자가 ".py"인 파일로 저장한다.
연습으로 py4inf 이름의 폴더를 바탕화면에 생성한다.
폴더명을 짧게 하고 폴더명과 파일명에 어떤 공백도 넣지 않는 것이 좋다.

첫 파이썬 프로그램을 다음과 같이 작성한다.




여러분의 이름으로 바꾸는 것을 제외하고, 파일을 바탕화면py4infprog1.py에 저장한다.

명령-줄(command line) 실행은 윈도우 버젼마다 다른다.


윈도우 비스타, 윈도우 7: 시작(Start)을 누루고, 명령어 실행 윈도우에서 단어 command를 입력하고 엔터를 친다.

윈도우-XP: 시작(Start)을 누루고,실행(Run)을 누루고, 대화창에서 단어 cmd를 입력하고 확인(OK)을 누른다.
지금 어느 폴더에 있는지를 말해주는 프롬프트 텍스트 윈도우에서 현재 위치를 확인할 수 있다.

Windows Vista and Windows-7: C:Userscsev

Windows XP: C:Documents and Settingscsev

이것이 "홈 디렉토리"다. 이제 다음 명령어를 사용해서 작성한 파이썬 프로그램을 저장한 폴더로 이동한다.




그리고 다음을 타이핑한다.




작성한 파일 목록을 보기 위해서 dir 명령어를 타이핑 할때, prog1.py이 보여야 한다.

프로그램을 실행하기 위해서, 단순히 명령 프롬프트에서 파일 이름을 타이핑하고 엔터를 친다.




NotePad++에서 파일을 편집하고, 저장하고, 명령줄로 돌아온다.
다시 명령줄 프롬프트에서 파일명을 타이핑해서 프로그램을 실행한다.

만약 명령줄 윈도우에서 혼동이 생기면, 단순하게 닫고 새로 시작한다.

힌트: 스크롤 백해서 이전에 입력한 명령을 다시 실행하기 위해서 "위쪽 화살표"를 명령줄에서 누른다.

NotePad++에서 환경설정 선호(preference)를 살펴보고 탭 문자가 공백 4개로 되도록 설정한다.
이 단순한 설정이 들여쓰기 오류를 찾는 수고를 많이 경감시켜 준다.

www.py4inf.com에서 파이썬 프로그램을 편집하고 실행하는 좀더 많은 정보를 얻을 수 있다.





매킨토쉬 상에서 파이썬 프로그래밍

이번 부록에서 일련의 단계를 거쳐서 매킨토쉬상에서 파이썬을 실행한다.
파이썬이 이미 매킨토쉬 운영시스템에 포함되어 있어서,
어떻게 파이썬 파일을 편집하는지와 터미널 윈도우에서 파이썬 프로그램을 실행하는 것을 학습할 필요가 있다.

파이썬 프로그램을 편집하고 실행하는데 취할 수 있는 서로 다른 많은 접근방법이 있다. 이것은 간단한 접근 방법중의 하나다.

먼저, 프로그래머 편집기를 설치할 필요가 있다.
Notepad나 윈도우 워드를 가지고 파이썬 프로그램을 편집할 필요는 없다.
프로그램은 "일반 텍스트(flat-text)" 파일이여서 텍스트 파일을 편집하는데 좋은 에디터만 필요하다.

매킨토쉬 시스템에 추천하고 싶은 편집기는 다음에서 다운받아 설치할 수 있는 TextWrangler다.

http://www.barebones.com/products/TextWrangler/

파이썬 프로그램을 생성하기 위해서 Applications 폴더 TextWrangler를 실행한다.

첫 파이썬 프로그램을 다음과 같이 작성한다.





여러분의 이름으로 바꾸는 것을 제외하고, 파일을 


바탕화면py4infprog1.py에 저장한다.

명령-줄(command line) 실행은 윈도우 버젼마다 다른다. py4inf로 데스크탑(Desktop) 폴더에 파일을 저장한다.
폴더명을 짧게 하고 폴더명과 파일명에 어떤 공백도 넣지 않는 것이 좋다.
폴더를 만들고 파일을 Desktoppy4infprog1.py 으로 저장한다.

터미널(Terminal) 프로그램을 실행. 가장 쉬운 방법은 화면 우측 상단의 Spotlight 아이콘(돋보기)를 누르고,
"terminal" 엔터치고 응용프로그램을 실행한다.

"홈 디렉토리"에서 시작한다. 터미널 윈도우에서 pwd 명령어를 타이핑해서 현재 디렉토리를 확인한다.




프로그램을 실행하기 위해서 파이썬 프로그램을 담고 있는 폴더에 있어야 한다.
cd 명령어를 사용해서 새 폴더로 이동하고 ls 명령어로 폴더의 파일 목록을 화면에 출력한다.




프로그램을 실행하기 위해서, 단순히 명령 프롬프트에서 python 명령어와 파일 이름을 타이핑하고 엔터를 친다.




TextWrangler에서 파일을 편집하고, 저장하고, 명령줄로 돌아온다.
다시 명령줄 프롬프트에서 파일명을 타이핑해서 프로그램을 실행한다.

만약 명령줄 윈도우에서 혼동이 생기면, 단순하게 닫고 새로 시작한다.

힌트: 스크롤 백해서 이전에 입력한 명령을 다시 실행하기 위해서 "위쪽 화살표"를 명령줄에서 누른다.

TextWrangler에서 환경설정 선호(preference)를 살펴보고 탭 문자가 공백 4개로 되도록 설정한다.
이 단순한 설정이 들여쓰기 오류를 찾는 수고를 많이 경감시켜 준다.

www.py4inf.com에서 파이썬 프로그램을 편집하고 실행하는 좀더 많은 정보를 얻을 수 있다.




공헌(contribution)

"정보교육을 위한 파이썬"에 공헌하신 분 목록

Bruce Shields 초기 초안을 편집,
Sarah Hegge,
Steven Cherry,
Sarah Kathleen Barbarow,
Andrea Parker,
Radaphat Chongthammakun,
Megan Hixon,
Kirby Urner,
Sarah Kathleen Barbrow,
Katie Kujala,
Noah Botimer,
Emily Alinder,
Mark Thompson-Kular,
James Perry,
Eric Hofer,
Eytan Adar,
Peter Robinson,
Deborah J. Nelson,
Jonathan C. Anthony,
Eden Rassette,
Jeannette Schroeder,
Justin Feezell,
Chuanqi Li,
Gerald Gordinier,
Gavin Thomas Strassel,
Ryan Clement,
Alissa Talley,
Caitlin Holman,
Yong-Mi Kim,
Karen Stover,
Cherie Edmonds,
Maria Seiferle,
Romer Kristi D. Aranas (RK),
Grant Boyer,
Hedemarrie Dussan,




"Think Python" 서문


"Think Python" 특이한 역사

(Allen B. Downey)

1999년 1월 자바로 프로그램 기초 과목 수업을 준비하고 있었다.
세번에 걸쳐서 수업을 했지만 좌절하고 있었다.
학급에서 낙제비율이 무척이나 높았고, 정상적으로 이수한 학생의 전반적인 성취도가 무척 낮았다.

목격한 여러 문제점 중의 하는 책이다.
자바 책이 너무 방대하고 불필요하게 상세한 정보가 너무 많았고 어떻게 프로그램을 작성하는지에 대한
높은 수준의 지침은 부족했다.
학생들 모두 뚜껑문 효과(trapdoor effect)로 고생했다. 즉, 쉽게 시작하고, 점진적으로 나아가지고나서 5장 주변에 
하위권 학생이 떨어져 나가는 것이다. 너무 많은 새로운 교재를 너무 빨리 학습해야했고,
저자는 학기의 나머지를 수습하는데 사용했다.

첫 수업시작하기 2주일 전에 직접 책을 쓰기로 마음먹었다.
목표는 다음과 같다.



짧게 한다. 읽지 않는 50 페이지보다 읽히는 10 페이지가 더 낫다.

용어에 주의한다. 전문용어를 최소하하고 첫번째 사용에 각 용어를 정의한다.

점진적으로 구축해 나간다. 뚜껑문 효과를 피하기 위해서, 가장 어려운 주제를 잡고 일련의 작은 단계로 쪼갠다.

프로그래밍 언어보다 프로그래밍에 집중한다. 최소한의 유용한 자바 부분을 포함하고 나머지는 생략한다.

제목이 필요했고, 즉흥적으로 어떻게 컴퓨터 과학자처럼 생각하기 (How to Think Like a Computer Scientist)로 정했다.

첫 버젼은 엉성했지만 실질적으로 작동했다. 학생들은 읽고, 충분히 이해해서 수업시간을 어렵고, 흥미로운 주제에 좀더 시간을 쓸 수 있었고,
가장 중요한 것은 학생들이 실습을 하게된 것이다.

GNU 공개 문서 라이센스(GNU Free Documentation License)로 책을 배포해서 누구나 복사, 편집, 배포할 수 있게 했다.

다음에 멋진 일이 생겼다. 버지니아 고등학교 교사인 Jeff Elkner 선생님이 책을 채용해서 파이썬으로 번역했다.
Jeff Elkner 선생님은 번역본을 보내왔고, 책을 읽고서 파이썬을 공부하게 되는 좀 특이한 경험을 했다.

Jeff와 저자는 책을 개작했고 Chris Meyers가 사례 연구를 추가했다. 2001년 GNU 공개 문서 라이센스로 
How to Think Like a Computer Scientist: Learning with Python 제목으로 배포했다.
Green Tea Press 출판사에서 책을 출판해서 Amazon.com과 대학 서점을 통해서 제본된 책을 팔기 시작했다.
Green Tea Press 출판사의 다른 책들은 greenteapress.com에서 살펴볼 수 있다.

2003년 Olin College에서 수업을 시작해서 처음으로 파이썬을 가르치게 되었다.
자바와 비교하여 놀라웠다. 학생들이 덜 고생하고, 더 많이 배우고, 좀더 흥미로운 프로젝트를 수행하고, 전반적으로 훨씬 재미있게 되었다.

지난 5년 동안 책을 계속적으로 개발하고, 오류를 수정하고, 예제을 향상하고, 교재, 특히 연습문제를 추가했다.
2008년 대대적인 수정 작업을 시작했는데 동시에 차기 개정판에 관심을 보인 Cambridge University Press 편집자와 계약했다. 좋은 시점이다.

이 책을 즐기고, 컴퓨터 과학자처럼 적어도 약간은 프로그램하고 생각하는 것을 배우는데 도움이 되기를 바랍니다.


"Think Python" 감사의 글

(Allen B. Downey)
처음으로 가장 중요하게, Jeff Elkner에게 감사드린다. 자바 책을 파이썬으로 번역해서
이 프로젝트가 시작되게 했고 가장 좋아하는 언어가 된 파이썬을 소개해 주었다.

Chris Meyers에게도 감사의 말씀을 드린다. How to Think Like a Computer Scientist 책의 몇개 부분에 기여해 주셨다.

Jeff와 Chris와 협동작업을 할수 있게 만든 GNU 공개 문서 라이센스를 개발한 
자유 소프트웨어 재단(Free Software Foundation)에 감사한다.

또한, How to Think Like a Computer Scientist 책을 작업하신 Lulu의 편집자에게 감사한다.

책의 초기 버젼을 함께 작업한 모든 학생들과 수정과 제안을 보내주신 부론에 나온 모든 기여자에게 감사한다.

그리고, 집사람 Lisa, Green Tea Press 그리고 다른 모든 것에도 감사한다.

Allen B. Downey 

Needham MA


Allen Downey 
컴퓨터 과학 부교수
Franklin W. Olin College of Engineering


"Think Python" 공헌자 목록

(Allen B. Downey)

날카로운 눈과 사려깊은 100명 이상의 독자가 지난 몇년동안 수정사항과 제안을 보내주었다.
이 프로젝트의 공헌과 열정은 매우 큰 도움이었다.

이들 참여자로부터의 각각의 공헌의 본질에 대한 자세한 사항은 "Think Python" 본문에서 확인하세요.

Lloyd Hugh Allen,
Yvon Boulianne,
Fred Bremmer,
Jonah Cohen,
Michael Conlon,
Benoit Girard,
Courtney Gleason and Katherine Smith,
Lee Harr,
James Kaylin,
David Kershaw,
Eddie Lam,
Man-Yong Lee,
David Mayo,
Chris McAloon,
Matthew J. Moelter,
Simon Dicon Montford,
John Ouzts,
Kevin Parks,
David Pool,
Michael Schmitt,
Robin Shaw,
Paul Sleigh,
Craig T. Snydal,
Ian Thomas,
Keith Verheyden,
Peter Winstanley,
Chris Wrobel,
Moshe Zadka,
Christoph Zwerschke,
James Mayer,
Hayden McAfee,
Angel Arnal,
Tauhidul Hoque and Lex Berezhny,
Dr. Michele Alzetta,
Andy Mitchell,
Kalin Harvey,
Christopher P. Smith,
David Hutchins,
Gregor Lingl,
Julie Peters,
Florin Oprina,
D. J. Webre,
Ken,
Ivo Wever,
Curtis Yanko,
Ben Logan,
Jason Armstrong,
Louis Cordier,
Brian Cain,
Rob Black,
Jean-Philippe Rey at Ecole Centrale Paris,
Jason Mader at George Washington University made a number
Jan Gundtofte-Bruun,
Abel David and Alexis Dinno,
Charles Thayer,
Roger Sperberg,
Sam Bull,
Andrew Cheung,
C. Corey Capel,
Alessandra,
Wim Champagne,
Douglas Wright,
Jared Spindor,
Lin Peiheng,
Ray Hagtvedt,
Torsten Hubsch,
Inga Petuhhov,
Arne Babenhauserheide,
Mark E. Casida,
Scott Tyler,
Gordon Shephard,
Andrew Turner,
Adam Hobart,
Daryl Hammond and Sarah Zimmerman,
George Sass,
Brian Bingham,
Leah Engelbert-Fenton,
Joe Funke,
Chao-chao Chen,
Jeff Paine,
Lubos Pintes,
Gregg Lind and Abigail Heithoff,
Max Hailperin,
Chotipat Pornavalai,
Stanislaw Antol,
Eric Pashman,
Miguel Azevedo,
Jianhua Liu,
Nick King,
Martin Zuther,
Adam Zimmerman,
Ratnakar Tiwari,
Anurag Goel,
Kelli Kratzer,
Mark Griffiths,
Roydan Ongie,
Patryk Wolowiec,
Mark Chonofsky,
Russell Coleman,
Wei Huang,
Karen Barber,
Nam Nguyen,
Stephane Morin,
and
Paul Stoop.





저작권 세부정보

이 책은 크리에이티브 커먼즈 라이선스 3.0 (Creative Commons Attribution-NonCommercial-Share Alike 3.0)으로 인가되었다.
라이선스의 자세한 사항은 creativecommons.org/licenses/by-nc-sa/3.0/에 기재되어 있다.

책을 좀더 제약이 덜한 CC-BY-SA 라이선스로 인가하고자 했다. 하지만 불행하게도 몇몇 비양심적인 조직은
자유로이 인가된 책을 탐색하고, 찾아서 LuLu 나 CreateSpace 같은 POD(print-on-demand) 서비스로 사실상 변경없이 책을 출판하고 판매하려 했다.
고맙게도 CreateSpace에서 자유로이 인가된 저작물을 출판하려는 비저작권자에 대해서 실질적 저작권자에게 우선권을 주는 정책을 추가했다.
불행하게도 많은 POD(print-on-demand) 서비스가 있고 매우 적은 기관만이 CreateSpace같은 사려 깊은 정책을 가지고 있다.

유감스럽게도, 이 책을 복제하고 상업적으로 팔려는 사람이 있는 경우에 대비해서 구상청구권으로 NC 요소를 라이센스에 추가했다.
불행하게도 NC를 추가하는 것은 저자가 허가하고자 하는 이 교재의 사용을 제한한다.
그래서 상업적인 부분을 생각하는 사람을 위한 상황에서 이 책의 내용을 어떻게 사용하는지를 미리 사용허가를 주는
특별한 상황을 기술하기 위해서 이 문서에 섹션을 추가했다.


학습에 사용될 목적으로 전부 혹은 책의 일부를 제한된 숫자로 책을 출력한다면, 이 목적으로 CC-BY 라이센스로 사용가능하다.

대학교의 선생님이고, 영어가 아닌 다른 언어로 이 책을 번역하고 번역된 책으로 수업을 한다면,
저자와 연락하고 CC-BY-SA 라이센스로 번역의 출판에 허용된다. 특히, 상업적으로 번역된 책을 판매하도록 허락된다.
만약 책을 번역하고자 한다면, 저자와 연락해서 관련된 수업 교재를 가지고 번역할 수 있도록 확인하세요.
물론, 이들 조항이 충분하지 않다면 연락을 환영하고 승인을 요청할 수 있다. 
모든 경우에 명확히 추가로 가치가 있고 새로운 저작의 결과로 생기는 효익이 학생이나 선생님에게 있기만 하면
이 교재를 재사용하고 믹싱하는 승인은 주어질 것이다.

Charles Severance

www.dr-chuck.com

Ann Arbor, MI, USA

2013년 9월 9일








chapter 장


































 

Charles R. Severance, 번역: 이광춘




























